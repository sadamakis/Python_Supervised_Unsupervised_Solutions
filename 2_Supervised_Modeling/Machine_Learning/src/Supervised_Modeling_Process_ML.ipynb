{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b72f2e12-2190-41c7-850b-f056eeca57e2",
   "metadata": {},
   "source": [
    "## Machine Learning with Python\n",
    "\n",
    "\n",
    "### Template to build a machine learning model with Python\n",
    "1. This template contains detailed steps to preprocess raw data and convert it into model input\n",
    "2. This template uses Random Forest, Gradient Boosting Machine, and lightGBM algorithms\n",
    "\n",
    "### Steps to build a model:\n",
    "1. Load the libaries and data\n",
    "2. Data preprocessing  \n",
    "    i. Remove variables with high missing values percentage  \n",
    "    ii. Remove categorical variables with many levels  \n",
    "    iii. Convert categorical variables to binary variables  \n",
    "    iv. Split train and out of time samples  \n",
    "    v. Impute missing values  \n",
    "    vi. Remove variables based on Gini threshold  \n",
    "3. Train the model (currently supporting scikit-learn RF, scikit-learn GBM, H2O RF, lightGBM)  \n",
    "    i. Background feature selection  \n",
    "    ii. Grid search for hyperparameter optimization using the following methods: \\\n",
    "        a. iterative steps \\\n",
    "        b. search through fixed grid space \\\n",
    "        c. search through random grid space \\\n",
    "        d. search using Optuna \\\n",
    "    iii. Train machine learning model   \n",
    "    iv. Calculate feature importance  \n",
    "4. Evaluate model  \n",
    "    i. Produce evaluation report  \n",
    "    ii. Produce lifting table \\\n",
    "    iii. Produce evaluation graphs\n",
    "\n",
    "### Machine Learning Libaries:\n",
    "1. preprocessing\n",
    "2. feature_selection\n",
    "3. model_builder\n",
    "4. evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811a4d67-7a58-4876-809d-0d458d5af588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the notebook full screen\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d222af-b659-44f7-a1f2-bb9c11bac6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import importlib\n",
    "import sys \n",
    "\n",
    "if sys.version_info[:3] < (3,4):\n",
    "    os.getcdw()\n",
    "    code_dir = os.path.dirname(os.getcdw())\n",
    "    project_dir = os.path.dirname(os.path.dirname(os.getcdw()))\n",
    "    data_path = os.path.join(code_dir, \"data\")\n",
    "    functions_path = os.path.join(project_dir, \"functions\")\n",
    "else: \n",
    "    from pathlib import Path\n",
    "    current_directory = os.path.dirname(Path.cwd())\n",
    "    code_dir = os.path.dirname(os.path.dirname(current_directory))\n",
    "    project_dir = os.path.join(code_dir, \"2_Supervised_Modeling\\\\Machine_Learning\")\n",
    "    data_path = os.path.join(code_dir, \"2_Supervised_Modeling\\\\Machine_Learning\\\\data\")\n",
    "    functions_path = os.path.join(code_dir, 'functions')\n",
    "\n",
    "print(code_dir)\n",
    "print(project_dir)\n",
    "print(data_path)\n",
    "print(functions_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966fab56-a8c1-4c50-ae35-dd34729f319d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Python modules\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18ea93b-8af4-4f84-8ac5-66bdcd3b7bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path for the library\n",
    "import sys\n",
    "sys.path.insert(0, functions_path)\n",
    "import data_transformation as dtran\n",
    "import variable_reduction as vr\n",
    "import feature_elimination as fe\n",
    "import model_builder as mb\n",
    "import reports as rp\n",
    "import useful_functions as ufun\n",
    "from load_data import load_data\n",
    "import keras_functions as ks_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba69179b-c611-4e48-81b9-288704db5818",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7eb9ff6-092c-4045-b77b-cbfdd806f4a9",
   "metadata": {},
   "source": [
    "# Initialize the solution variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79160fd2-bbd1-4867-8e81-a5ce75059076",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(project_dir, 'data/input/Supervised_Modeling_Solution_Input_ML.json')) as f:\n",
    "    inputs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97136bd-4dae-4125-8fec-d25b1520dcb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba251a6-b67b-420e-a894-857de08b1b4b",
   "metadata": {},
   "source": [
    "## Essential parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b5b784-ed72-4cb5-8665-88243a6cb2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# String. Specify how to load the data. Options: csv, parq.\n",
    "Load_from = inputs[\"Load_from\"]\n",
    "# String. Specify the data location: this is the folder where the data for this project are saved. \n",
    "data_location = inputs[\"data_location\"]\n",
    "# String. Set the input data file. \n",
    "table_name = inputs[\"table_name\"]\n",
    "# Float. Number between 0-1 determining what percent of data to subsample. \n",
    "sample = float(inputs[\"sample\"])\n",
    "# String. Set the target variable name in the original dataset. \n",
    "target_variable_name = inputs[\"target_variable_name\"]\n",
    "# String. Set the weight variable name in the original dataset. If not avaulable, then provide \"None\".\n",
    "weight_variable_name = inputs[\"weight_variable_name\"]\n",
    "# String. Set the sample column that has sample information, e.g. train/test/OOT or segment information, and will be used to split the data in different samples\n",
    "# String. If this column does not exist, then provide \"None\".\n",
    "sample_variable_name = inputs[\"sample_variable_name\"]\n",
    "# String. Name of the amount/cost per record. If this column does not exist, then provide \"None\".\n",
    "amount_variable_name = inputs[\"amount_variable_name\"]\n",
    "# List of strings. Set the sub-sample values that are in the sample_variable_name field, e.f. for train/test data split and/or for different segments. \n",
    "# All samples defined in this parameters will be picked up by the solution and results will be created for these samples. \n",
    "# If sample column does not exist, then provide '[None]' (without quotes).\n",
    "sample_values = inputs[\"sample_values\"]\n",
    "# List. Provide the feature names for the numeric variables that will be used for modeling. \n",
    "original_candidate_variables_numeric = inputs[\"numeric_variables_modeling\"]\n",
    "# List. Provide the feature names for the character variables that will be used for modeling. \n",
    "original_candidate_variables_character = inputs[\"character_variables_modeling\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba96f056-7353-42c3-bb1f-1be4c34714a0",
   "metadata": {},
   "source": [
    "## Advanced parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4a8d27-1b3f-438e-8485-69d867daf5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Float. Takes values between 0 and 1. Used in 'select_missing_variables_to_drop' function. Variables with percentage missing values above this threshold will be \n",
    "# dropped from the rest of the process. \n",
    "select_missing_variables_to_drop_threshold = inputs[\"select_missing_variables_to_drop_threshold\"]\n",
    "# Integer. Used in 'character_classification' function. Character variables with more levels than this threshold will be dropped from the rest of the process. \n",
    "character_classification_threshold = inputs[\"character_classification_threshold\"]\n",
    "# Float. Used in the 'replace_outliers' function in the outlier removal section. This is the coefficient for Interquantile range. \n",
    "# It can be used to adjust how many outliers to replace; the higher the value the less outliers are replaced. \n",
    "iqr_coef = inputs[\"iqr_coef\"]\n",
    "# String. Used in 'impute_missing' class. Select the stratefy to impute the missing values. Current options are \"median\", \"mean\", \n",
    "# or a specific value without quotes, e.g. 0.\n",
    "impute_missing_imputation_strategy = inputs[\"impute_missing_imputation_strategy\"]\n",
    "# Float. Variables with Gini coefficient below this threshold will be dropped from the reamained of the analysis. \n",
    "gini_threshold = inputs[\"gini_threshold\"]\n",
    "# Integer. Number of top features based on feature importance to apply a first screening using Random Forest. \n",
    "rf_initial_screening_var_threshold = inputs[\"rf_initial_screening_var_threshold\"]\n",
    "# Integer. Number of features to remain using backward selection with random forest model: In each step, a feature that has the minimum Gini is selected to be removed. \n",
    "rf_backward_selection_var_threshold = inputs[\"rf_backward_selection_var_threshold\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e438bde-3333-478d-93e1-5df10740b2b8",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24b4d66-7b86-4f49-a97a-9dfcd556b0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full = load_data(method = Load_from, \n",
    "                     data_path = data_location, \n",
    "                     table_name = table_name, \n",
    "                     sample = sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a47f3de-ccf2-45ee-8453-5d06735fc594",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_full.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3680f4f7-c64b-429c-bb80-12a0c862b584",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929d4911-7cc7-428f-8d88-45aaa2ed195f",
   "metadata": {},
   "source": [
    "# Create the Weight, Sample and Amount variables, if not available in the input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee51b3c-dd87-4976-b7e3-b87a8f102f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the weight variable, if it doesn't exist.\n",
    "data_full, weight_variable_name_solution = dtran.weight_var_assignment(input_data = data_full, \n",
    "                                                                                     weight_variable = weight_variable_name)\n",
    "\n",
    "# Create the sample variable, if it doesn't exist.\n",
    "data_full, sample_values_solution, sample_variable_name_solution = dtran.sample_var_assignment(input_data = data_full, \n",
    "                                                                                        sample_variable = sample_variable_name,\n",
    "                                                                                        sample_values = sample_values)\n",
    "\n",
    "# Create the amount variable, if it doesn't exist.\n",
    "data_full, amount_variable_name_solution = dtran.amount_var_assignment(input_data = data_full, \n",
    "                                                                                     amount_variable = amount_variable_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d453e1f6-cd5e-4424-a1ac-9c70101eb66c",
   "metadata": {},
   "source": [
    "# Subset the dataset to use only the samples selected by 'sample values'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9918d7c-1739-4bb7-951a-156691418118",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full = data_full[data_full[sample_variable_name_solution].isin(sample_values_solution)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05fa0d3-f687-4ff8-81b1-4bdcd384cb5f",
   "metadata": {},
   "source": [
    "# Convert variable data types based on user information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3d5749-f47c-448d-b01a-271a922f7e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert character variables\n",
    "data_full, character_variables_list = dtran.convert_character_var(input_data = data_full, \n",
    "                                                        character_variables = original_candidate_variables_character,\n",
    "                                                        sample_variable = sample_variable_name_solution)\n",
    "\n",
    "# Convert numeric variables\n",
    "data_full, numeric_variables_list = dtran.convert_numeric_var(input_data = data_full, \n",
    "                                                        numeric_variables = original_candidate_variables_numeric,\n",
    "                                                        weight_variable = weight_variable_name_solution, \n",
    "                                                        amount_variable = amount_variable_name_solution, \n",
    "                                                        target_variable = target_variable_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec215d6e-1a0a-4cbb-9b28-85ba476a96f6",
   "metadata": {},
   "source": [
    "# Data quality report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e4239d-c804-4566-9bab-80fba4315dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder, if it doesn't exist\n",
    "ufun.create_folder(data_path = data_path, \n",
    "                   folder_name = 'output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee1bd6e-5c3a-4e03-a55d-200083ab4b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "dq = rp.dq_report(input_data = data_full, \n",
    "                data_path = data_path, \n",
    "                variables = character_variables_list + numeric_variables_list, \n",
    "                weight_variable = weight_variable_name_solution, \n",
    "                dq_report_file = 'data_quality_report.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c670fb-a527-4ca5-aca8-59694138182f",
   "metadata": {},
   "source": [
    "# Split sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15870ef1-fbea-4446-af80-8aa295e65658",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, sample_values_dict = dtran.split_sample_data(\n",
    "    input_data=data_full, \n",
    "    sample_values_solution=sample_values_solution, \n",
    "    sample_variable_name_solution=sample_variable_name_solution\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0229ddf9-035c-4e08-93aa-9c4fd57ce689",
   "metadata": {},
   "source": [
    "# Set the original candidate variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecc5066-89ef-4e75-8dd0-dc0eab583f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_candidate_variables = original_candidate_variables_character + original_candidate_variables_numeric\n",
    "print(ufun.color.BLUE + 'Original candidate variables: ' + ufun.color.END + str(original_candidate_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f35b0e-1824-4b26-a14b-34473e054d44",
   "metadata": {},
   "source": [
    "# Remove variables with high missing values percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e79733-2d94-49d9-bc96-ec499df6bdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables excluded from the non-predictive features: keys, target, sample, etc\n",
    "excluded_variables = [x for x in data['data_{}'.format(sample_values_solution[0])].columns if x not in original_candidate_variables]\n",
    "print(ufun.color.BLUE + 'Variables to be excluded: ' + ufun.color.END + str(excluded_variables))\n",
    "print()\n",
    "# Produce and save the missing values table to review\n",
    "missing_variables_table, missing_variables = vr.missing_values_vars(\n",
    "    sample_values_dict=sample_values_dict, \n",
    "    data_path=data_path, \n",
    "    input_data=data, \n",
    "    weight_variable_name_solution=weight_variable_name_solution, \n",
    "    select_missing_variables_to_drop_threshold=select_missing_variables_to_drop_threshold\n",
    "    )\n",
    "# Create the variables to remove: non-predictors + variables with too many missing information\n",
    "excluded_variables = excluded_variables + missing_variables\n",
    "print(ufun.color.BLUE + 'Variables to remove from the remainder of the analysis: ' + ufun.color.END + str(excluded_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0c0e14-df5c-4af2-b574-e65b13c658b5",
   "metadata": {},
   "source": [
    "# Remove character variables with many levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64546dd3-f228-401b-976c-ac6cd142e877",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_char_vars_levels, excl_char_vars = vr.character_var_levels(\n",
    "    input_data = data, \n",
    "    data_path = data_path, \n",
    "    sample_values_solution = sample_values_solution,\n",
    "    excluded_variables = excluded_variables, \n",
    "    character_classification_threshold = character_classification_threshold\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59351c5b-d5af-47f5-9247-92c3a1ad4a95",
   "metadata": {},
   "source": [
    "# Outlier replacement for numeric variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35df4d7d-3ef1-4f98-9735-39fa8c187267",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outlier_variables = [i for i in original_candidate_variables_numeric if i not in excluded_variables]\n",
    "data_full = dtran.replace_outliers(\n",
    "    input_data = data_full, \n",
    "    variables = outlier_variables, \n",
    "    weight_variable = weight_variable_name_solution, \n",
    "    data_path = data_path, \n",
    "    outlier_info_file = 'outlier_info.csv', \n",
    "    iqr_coef = iqr_coef\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf04806-73b8-4aa9-bd85-4b8642fb173b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split sample data\n",
    "data, temp_dict = dtran.split_sample_data(\n",
    "    input_data=data_full, \n",
    "    sample_values_solution=sample_values_solution, \n",
    "    sample_variable_name_solution=sample_variable_name_solution\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597c12b4-cd50-41b2-bcb7-d5cf21e704f0",
   "metadata": {},
   "source": [
    "# Convert categorical variables to binary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8979d9e-35ec-4711-89ca-0e5667ef5675",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full = dtran.character_to_binary(\n",
    "    input_data = data_full, \n",
    "    input_variable_list = keep_char_vars_levels, \n",
    "    drop = 'last', # Specifies which value to drop from the one hot encoder. None will return binary variables for all categories. 'first' will drop the most populated category. 'last' will drop the less populated category. \n",
    "    protected_class_valid_values = None # Specifies accepted values for the protected class column. For non-protected class conversions use 'None'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbba151-f45b-4056-a21e-75ed156dd986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split sample data\n",
    "data, temp_dict = dtran.split_sample_data(\n",
    "    input_data=data_full, \n",
    "    sample_values_solution=sample_values_solution, \n",
    "    sample_variable_name_solution=sample_variable_name_solution\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bd46ba-2885-4baa-80da-b58efe6a85ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep all numeric variables, including those that were one-hot encoded\n",
    "keep_num_vars = ufun.identify_numeric_variables(input_data=data['data_{}'.format(sample_values_solution[0])])\n",
    "keep_num_vars = [x for x in keep_num_vars if x not in excluded_variables]\n",
    "print('Keeping the following variables: ', keep_num_vars)\n",
    "print(len(keep_num_vars))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a899eb-a3a8-4747-ad54-0c04fdbfdd1e",
   "metadata": {},
   "source": [
    "# Impute missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485a4c89-ac57-45b7-8533-ef232b067eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_with_missing_dict = vr.select_missing_variables_to_drop_dict(\n",
    "    sample_values_dict = sample_values_dict, \n",
    "    data_path = data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd80a1e0-5945-41fd-8300-fa03fd8f3e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numeric features with missing values. Imputation will be applied to only these features, in order to improve the performance of the code. \n",
    "variables_with_missing = list(dict.fromkeys(sum(variables_with_missing_dict.values(), [])))\n",
    "num_variables_with_missing = [i for i in keep_num_vars if i in variables_with_missing]\n",
    "num_variables_with_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85479abb-c97f-42e2-82f4-6bb99a68c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values\n",
    "start_time = time.time()\n",
    "impute_missing = dtran.impute_missing(\n",
    "        variables = num_variables_with_missing, \n",
    "        imputation_strategy = impute_missing_imputation_strategy)\n",
    "impute_missing.imputation_fit_weight(\n",
    "        input_data = data['data_{}'.format(sample_values_solution[0])], \n",
    "        weight_variable = weight_variable_name_solution)\n",
    "\n",
    "for i, j in sample_values_dict.items():\n",
    "    impute_missing.imputation_transform(input_data = data['data_{}'.format(i)])\n",
    "\n",
    "print('This code took %.2fs. to run'%(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79673b4-8cc8-430e-92ab-b6eb2e7273af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check missing values for imputed variables\n",
    "for i, j in sample_values_dict.items():\n",
    "    start_time = time.time()\n",
    "    print(ufun.color.BOLD + ufun.color.PURPLE + ufun.color.UNDERLINE + 'SAMPLE ' + i + ufun.color.END)\n",
    "\n",
    "    if num_variables_with_missing != []:\n",
    "        print(data['data_{}'.format(i)][num_variables_with_missing].apply\n",
    "              (lambda x: (sum(data['data_{}'.format(i)][x.isnull()][weight_variable_name_solution])\n",
    "                /sum(data['data_{}'.format(i)][weight_variable_name_solution])) * 100, axis=0).sort_values(ascending=False))\n",
    "    else: \n",
    "        print('There are no variables with missing values to impute')\n",
    "\n",
    "    print('This code took %.2fs. to run'%(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be7c26c-5ee9-4765-be57-13988e861dfd",
   "metadata": {},
   "source": [
    "# Drop numeric variables with only one value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a123358d-39c6-44c8-a727-d0f1321fba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_num_vars_one_v = vr.keep_num_variables_one_value(\n",
    "    keep_num_vars = keep_num_vars, \n",
    "    data_path = data_path, \n",
    "    dq_report = 'data_quality_report.csv'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e89279-4747-42db-ac18-7a20c778b566",
   "metadata": {},
   "source": [
    "# Drop variables based on low Gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3187162d-65be-451c-89c4-db49772a15e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gini_table = fe.gini_values_weight(feats = keep_num_vars_one_v, \n",
    "                   input_data = data['data_{}'.format(sample_values_solution[0])], \n",
    "                   target_variable = target_variable_name, \n",
    "                   weight_variable = weight_variable_name_solution, \n",
    "                   data_path = data_path, \n",
    "                   gini_info_file = 'gini_info.csv', \n",
    "                   n_bands = 10)\n",
    "keep_num_vars_gini = list(gini_table.loc[gini_table['Gini coefficient'] >= gini_threshold, 'variable'].values)\n",
    "print(ufun.color.PURPLE + 'Keeping the following variables with Gini > ' + str(gini_threshold) + ': ' + ufun.color.END + str(keep_num_vars_gini))\n",
    "print(len(keep_num_vars_gini))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f106e6b-a28b-486b-9db6-8153db60b069",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff53337-b09f-43fc-a831-d626b87fb3fa",
   "metadata": {},
   "source": [
    "## Feature selection RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d527f2d3-caca-4d66-93fa-541b571712de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide input data to feature selection class\n",
    "select = fe.SelectBest_weight(df=data['data_{}'.format(sample_values_solution[0])], \n",
    "                              target=target_variable_name, \n",
    "                              weight=weight_variable_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badb2b02-8b58-4fd4-9755-923d2a55b2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top features to apply a first screening\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model_rf = RandomForestClassifier(n_estimators=200, max_depth=5, random_state=1234, n_jobs=6)\n",
    "rf_importance, feats_best_rf = select.top_rf_feat(feats=keep_num_vars_gini, model=model_rf, n=rf_initial_screening_var_threshold)\n",
    "print('The top Random Forest features are: \\n', feats_best_rf)\n",
    "rf_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e145e8-831e-4660-8dcf-1871da1d2da6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Backward selection with random forest model: In each step a feature with the minimum Gini is selected to be removed.\n",
    "feats_best_rf_back = select.backward_recur(feats=feats_best_rf, \n",
    "                                           oos=data['data_{}'.format(sample_values_solution[1])], \n",
    "                                           model=model_rf, \n",
    "                                           min_feats=rf_backward_selection_var_threshold, \n",
    "                                           classification=True)\n",
    "print(feats_best_rf_back)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b1cea0-0bab-4fae-b948-884a06f0e9ea",
   "metadata": {},
   "source": [
    "## Grid search and train RF machine learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a03fec-1131-4b2d-bbc2-9d5156b79a72",
   "metadata": {},
   "source": [
    "### Grid search based on iterative step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1ee559-0f51-4c69-a5ae-e2da874e3967",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameter tuning: The process works as follows:\n",
    "# 1) Iterate n_estimators (grid) for the fixed values of max_depth and max_features (params). Select the value of n_estimators that minimizes the loss function. \n",
    "# 2) Keep the n_estimators value from the previous step. Iterate max_depth (grid) for the fixed value of max_features (params). Select the value of max_depth that minimizes the loss function. \n",
    "# 3) Keep the max_depth value from the previous step. Iterate max_features (grid). Select the value of max_features that minimizes the loss function. \n",
    "\n",
    "import math\n",
    "# 18 models in total\n",
    "grid = {'n_estimators':[10, 50, 80, 100, 200, 400],  # Number of trees, 100-500 is usually sufficient /\n",
    "       'max_depth':[2, 5, 10, 20],  # Depth of the tree, 5-6 is usually default\n",
    "       'max_features':[*range(1,len(feats_best_rf_back),math.ceil(len(feats_best_rf_back)/5))],  # Number of variables randomly sampled as candidates at each split, sqrt(p) is usually the default\n",
    "       'min_samples_leaf': [1, 10, 100]} # Minimum number of observations in each leaf, 10 is usually default\n",
    "\n",
    "params = {'n_estimators': 100, 'max_depth':10, 'max_features':math.ceil(math.sqrt(len(feats_best_rf_back))), 'min_samples_leaf':10}\n",
    "\n",
    "opt_params_iterative, loss = mb.step_search_weight(estimator=RandomForestClassifier, \n",
    "                                                   params=params, \n",
    "                                                   grid=grid, \n",
    "                                                   target=target_variable_name, \n",
    "                                                   weight=weight_variable_name, \n",
    "                                                   dev=data['data_{}'.format(sample_values_solution[0])], \n",
    "                                                   val=data['data_{}'.format(sample_values_solution[1])], \n",
    "                                                   keep=feats_best_rf_back)\n",
    "print('\\n Best Parameters')\n",
    "print(opt_params_iterative)\n",
    "print('Loss: ', loss)\n",
    "\n",
    "# Load RF machine leanring library\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "\n",
    "# Define the random forest model\n",
    "rf = RandomForestClassifier(n_estimators=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='n_estimators'][0], \n",
    "                            max_depth=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='max_depth'][0], \n",
    "                            max_features=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='max_features'][0], \n",
    "                            min_samples_leaf=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='min_samples_leaf'][0], \n",
    "                            random_state=0)\n",
    "\n",
    "# Train the model\n",
    "rf_model = mb.fit_model_weight(df=data['data_{}'.format(sample_values_solution[0])], \n",
    "                               feats=feats_best_rf_back, \n",
    "                               target=target_variable_name, \n",
    "                               weight=weight_variable_name, \n",
    "                               model=rf, \n",
    "                               model_name=data_path + '/output/' + 'rf_iterative.pkl')\n",
    "\n",
    "# Compute the score for the OOT data\n",
    "for k in data.keys():\n",
    "    data[k].loc[:, 'rf_score_iterative_numeric'] = rf_model.predict_proba(data[k].loc[:, feats_best_rf_back].values)[:, 1]\n",
    "    data[k].loc[:, 'rf_score_iterative_binary'] = list(map(round, data[k].loc[:, 'rf_score_iterative_numeric']))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c8f436-a4a8-40dc-9b51-84fecf6805c6",
   "metadata": {},
   "source": [
    "### Grid search based on RandomizedSearchCV - also uses cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e904534-c42e-4494-8937-eb2273a73f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1,296 models in total\n",
    "grid = {'n_estimators':[10, 50, 80, 100, 200, 400],  # Number of trees, 100-500 is usually sufficient /\n",
    "       'max_depth':[2, 3, 4, 5, 6, 7],  # Depth of the tree, 5-6 is usually default\n",
    "       'max_features':[*range(15,21,1)],  # Number of variables randomly sampled as candidates at each split, sqrt(p) is usually the default\n",
    "       'min_samples_leaf': [1, 10, 100], # Minimum number of observations in each leaf, 10 is usually default\n",
    "       'criterion': ['gini', 'entropy']} # The function to measure the quality of a split\n",
    "        \n",
    "rf_grid_random_search_model = mb.grid_search_cv(\n",
    "    n_splits = 2, # Number of cross-validation splits\n",
    "    classifier = RandomForestClassifier, # Classifier name, e.g. RandomForestClassifier\n",
    "    keras_function = None, # Define Keras function. If Keras is not used, then leave this parameter blank\n",
    "    grid_params = grid, # Grid space\n",
    "    dev_df = data['data_{}'.format(sample_values_solution[0])],  # Development sample that this will analysis will be performed\n",
    "    feats = feats_best_rf_back, # List of predictor names\n",
    "    target = target_variable_name, # Target variable name\n",
    "    weight_variable = weight_variable_name, # Weight variable name\n",
    "    randomized_search = True, # Set to True if randomized grid search will be performed, or to False if exhaustive grid search will be performed\n",
    "    n_random_grids = 100, # Number of grid searches when randomized_search=True. If randomized_search=False, then this parameter is not applicable\n",
    "    random_state = None, # If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.\n",
    "    n_jobs = 8 # Number of jobs to run in parallel\n",
    ")\n",
    "\n",
    "# Save the best parameters\n",
    "opt_params_random = rf_grid_random_search_model.best_params_\n",
    "print('The best hyperparameter set is:', opt_params_random)\n",
    "print('Mean loss function for cross-validation test data: ', -rf_grid_random_search_model.cv_results_['mean_test_score'][rf_grid_random_search_model.best_index_])\n",
    "print('Standard deviation loss function for cross-validation test data: ', rf_grid_random_search_model.cv_results_['std_test_score'][rf_grid_random_search_model.best_index_])\n",
    "#print('Mean Gini for OOT data', 2*rf_grid_random_search_model.score(oos[feats_best_rf_back], oos[target_variable_name])-1)\n",
    "\n",
    "rp.plot_cross_validation_score(model=rf_grid_random_search_model)\n",
    "\n",
    "# Compute the score for the OOT data\n",
    "for k in data.keys():\n",
    "    data[k].loc[:, 'rf_score_random_numeric'] = rf_grid_random_search_model.predict_proba(data[k][feats_best_rf_back].values)[:, 1]\n",
    "    data[k].loc[:, 'rf_score_random_binary'] = list(map(round, data[k].loc[:, 'rf_score_random_numeric']))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096b938e-38d3-48de-b226-f20233fd834d",
   "metadata": {},
   "source": [
    "### Grid search based on GridSearchCV - also uses cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68680a7b-4dd9-4670-9fb1-bd8448147bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 72 models in total\n",
    "grid = {'n_estimators':[100, 200],  # Number of trees, 100-500 is usually sufficient /\n",
    "       'max_depth':[3, 5, 7],  # Depth of the tree, 5-6 is usually default\n",
    "       'max_features':[18, 21, 24],  # Number of variables randomly sampled as candidates at each split, sqrt(p) is usually the default\n",
    "       'min_samples_leaf': [1, 10], # Minimum number of observations in each leaf, 10 is usually default\n",
    "       'criterion': ['gini', 'entropy']} # The function to measure the quality of a split\n",
    "        \n",
    "rf_grid_fixed_search_model = mb.grid_search_cv(\n",
    "    n_splits = 2, # Number of cross-validation splits\n",
    "    classifier = RandomForestClassifier, # Classifier name, e.g. RandomForestClassifier\n",
    "    keras_function = None, # Define Keras function. If Keras is not used, then leave this parameter blank\n",
    "    grid_params = grid, # Grid space\n",
    "    dev_df = data['data_{}'.format(sample_values_solution[0])],  # Development sample that this will analysis will be performed\n",
    "    feats = feats_best_rf_back, # List of predictor names\n",
    "    target = target_variable_name, # Target variable name\n",
    "    weight_variable = weight_variable_name, # Weight variable name\n",
    "    randomized_search = False, # Set to True if randomized grid search will be performed, or to False if exhaustive grid search will be performed\n",
    "    n_random_grids = 1, # Number of grid searches when randomized_search=True. If randomized_search=False, then this parameter is not applicable\n",
    "    random_state = None, # If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.\n",
    "    n_jobs = 8 # Number of jobs to run in parallel\n",
    ")\n",
    "\n",
    "# Save the best parameters\n",
    "opt_params_fixed = rf_grid_fixed_search_model.best_params_\n",
    "print('The best hyperparameter set is:', opt_params_fixed)\n",
    "print('Mean loss function for cross-validation test data: ', -rf_grid_fixed_search_model.cv_results_['mean_test_score'][rf_grid_fixed_search_model.best_index_])\n",
    "print('Standard deviation loss function for cross-validation test data: ', rf_grid_fixed_search_model.cv_results_['std_test_score'][rf_grid_fixed_search_model.best_index_])\n",
    "#print('Mean Gini for OOT data', 2*rf_grid_fixed_search_model.score(oos[feats_best_rf_back], oos[target_variable_name], sample_weight=oos[weight_variable_name])-1)\n",
    "\n",
    "rp.plot_cross_validation_score(model=rf_grid_fixed_search_model)\n",
    "\n",
    "# Compute the score for the OOT data\n",
    "for k in data.keys():\n",
    "    data[k].loc[:, 'rf_score_fixed_numeric'] = rf_grid_fixed_search_model.predict_proba(data[k][feats_best_rf_back].values)[:, 1]\n",
    "    data[k].loc[:, 'rf_score_fixed_binary'] = list(map(round, data[k].loc[:, 'rf_score_fixed_numeric']))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719cdd5f-3482-42fa-85d3-eed5b66cce29",
   "metadata": {},
   "source": [
    "### Grid search based on Optuna - also uses cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e754f0f-7b28-4f2b-ab90-b48c0dddb142",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rf_optuna_search_model = mb.grid_search_optuna(\n",
    "    classifier = RandomForestClassifier, # Classifier name, e.g. RandomForestClassifier\n",
    "    grid_params = {\n",
    "        'n_estimators': ([10, 400], 'int'),  # Number of trees, 100-500 is usually sufficient /\n",
    "           'max_depth': ([2, 7], 'int'),  # Depth of the tree, 5-6 is usually default\n",
    "           'max_features': ([\"sqrt\", \"log2\"], 'cat'),  # Number of variables randomly sampled as candidates at each split, sqrt(p) is usually the default\n",
    "           'min_samples_leaf': ([0.01, 0.04], 'float'), # Minimum number of observations in each leaf, 10 is usually default\n",
    "           'min_samples_split': ([0.01, 0.04], 'float'), # The minimum number of observations required to split an internal node\n",
    "        # If int then min_samples_split is the minimum number of observations\n",
    "        # If float then then minimum number of observations is a fraction and ceil(min_samples_split * n_samples) \n",
    "           'criterion': (['gini', 'entropy'], 'cat')}, # The function to measure the quality of a split\n",
    "    dev_df = data['data_{}'.format(sample_values_solution[0])], # dev_df: Development sample that this will analysis will be performed\n",
    "    feats = feats_best_rf_back, # List of predictor names\n",
    "    target = target_variable_name, # Target variable name\n",
    "    weight_variable = weight_variable_name, # Weight variable name\n",
    "    random_state = 42, # If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.\n",
    "    n_splits_kfold = 2, # Number of cross-validation splits\n",
    "    n_repeats_kfold = 1, # Number that KFold cross-validation will be repeated\n",
    "    n_random_grids = 100, # Number of grid searches from Optuna. The more searches, the longer that the algorithm will take to complete. \n",
    "    timeout = 100000, # Time in seconds that Optuna will stop\n",
    "    n_jobs = -1 # Number of jobs to run in parallel\n",
    "    )\n",
    "\n",
    "rf_optuna_search_model.optimize()\n",
    "\n",
    "# Save the best parameters\n",
    "opt_params_optuna = rf_optuna_search_model.best_params()\n",
    "opt_loss_optuna = rf_optuna_search_model.best_score()\n",
    "print('Best Hyperparameters (Optuna): ', opt_params_optuna)\n",
    "print('Mean loss function for cross-validation (Optuna): ', opt_loss_optuna)\n",
    "\n",
    "rf_optuna_search_model.train_best_model()\n",
    "\n",
    "# Compute the score for the OOT data\n",
    "for k in data.keys():\n",
    "    data[k].loc[:, 'rf_score_optuna_numeric'] = rf_optuna_search_model.predict_probabilities(X_test = data[k][feats_best_rf_back])[:, 1]\n",
    "    data[k].loc[:, 'rf_score_optuna_binary'] = list(map(round, data[k].loc[:, 'rf_score_optuna_numeric']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6968058a-7131-406d-b9bf-ef3332e964bc",
   "metadata": {},
   "source": [
    "## Calculate RF feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d491b05-69e5-4046-a0b1-94a26ce2875a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load RF machine leanring library\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "\n",
    "# Define the random forest model\n",
    "opt_params = opt_params_optuna\n",
    "rf = RandomForestClassifier(n_estimators=[x[1] for x in list(opt_params.items()) if x[0]=='n_estimators'][0], \n",
    "                            max_depth=[x[1] for x in list(opt_params.items()) if x[0]=='max_depth'][0], \n",
    "                            max_features=[x[1] for x in list(opt_params.items()) if x[0]=='max_features'][0], \n",
    "                            min_samples_leaf=[x[1] for x in list(opt_params.items()) if x[0]=='min_samples_leaf'][0], \n",
    "                            min_samples_split=[x[1] for x in list(opt_params.items()) if x[0]=='min_samples_split'][0], \n",
    "                            criterion=[x[1] for x in list(opt_params.items()) if x[0]=='criterion'][0], \n",
    "                            random_state=0)\n",
    "\n",
    "# Train the model\n",
    "rf_model = mb.fit_model_weight(data['data_{}'.format(sample_values_solution[0])], \n",
    "                               feats_best_rf_back, \n",
    "                               target_variable_name, \n",
    "                               weight_variable_name, \n",
    "                               rf, \n",
    "                               data_path + '/output/' + 'rf.pkl')\n",
    "\n",
    "# Calculate feature importance\n",
    "feat_imprtnce_dictnry = mb.feature_imp(rf_model, feats_best_rf_back, data_path, 'Feature_importance_RF.csv')\n",
    "display(feat_imprtnce_dictnry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82dd6ff-fe98-4eac-8339-5ac44181ce15",
   "metadata": {},
   "source": [
    "## Produce RF reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f52c19-ec8a-4640-9854-a2cd16142210",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_iterative_report_class = rp.binary_regression_report(\n",
    "    predictions_dictionary = data, \n",
    "    target_variable = target_variable_name, \n",
    "    predicted_score_numeric = 'rf_score_iterative_numeric', \n",
    "    amount_variable_name = amount_variable_name_solution, \n",
    "    weight_variable_name = weight_variable_name_solution, \n",
    "    sample_values_dict = sample_values_dict, \n",
    "    select_top_percent = 100, \n",
    "    n_bands = 10, \n",
    "    rows = 10, \n",
    "    data_path = data_path\n",
    "    )\n",
    "rf_iterative_eval = rf_iterative_report_class.get_evaluation(predicted_score_binary = 'rf_score_iterative_binary',\n",
    "                                                            filename = 'evaluation_metrics_RF_iterative.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff95bb2f-454f-4cc6-b051-ce1e9c25d75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random_report_class = rp.binary_regression_report(\n",
    "    predictions_dictionary = data, \n",
    "    target_variable = target_variable_name, \n",
    "    predicted_score_numeric = 'rf_score_random_numeric', \n",
    "    amount_variable_name = amount_variable_name_solution, \n",
    "    weight_variable_name = weight_variable_name_solution, \n",
    "    sample_values_dict = sample_values_dict, \n",
    "    select_top_percent = 100, \n",
    "    n_bands = 10, \n",
    "    rows = 10, \n",
    "    data_path = data_path\n",
    "    )\n",
    "rf_random_eval = rf_random_report_class.get_evaluation(predicted_score_binary = 'rf_score_random_binary',\n",
    "                                                            filename = 'evaluation_metrics_RF_random.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1566dc66-48dc-4e15-95b8-a988ff981faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_fixed_report_class = rp.binary_regression_report(\n",
    "    predictions_dictionary = data, \n",
    "    target_variable = target_variable_name, \n",
    "    predicted_score_numeric = 'rf_score_fixed_numeric', \n",
    "    amount_variable_name = amount_variable_name_solution, \n",
    "    weight_variable_name = weight_variable_name_solution, \n",
    "    sample_values_dict = sample_values_dict, \n",
    "    select_top_percent = 100, \n",
    "    n_bands = 10, \n",
    "    rows = 10, \n",
    "    data_path = data_path\n",
    "    )\n",
    "rf_fixed_eval = rf_fixed_report_class.get_evaluation(predicted_score_binary = 'rf_score_fixed_binary',\n",
    "                                                            filename = 'evaluation_metrics_RF_fixed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568a87f5-319e-4615-9de6-bedef10df96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_optuna_report_class = rp.binary_regression_report(\n",
    "    predictions_dictionary = data, \n",
    "    target_variable = target_variable_name, \n",
    "    predicted_score_numeric = 'rf_score_optuna_numeric', \n",
    "    amount_variable_name = amount_variable_name_solution, \n",
    "    weight_variable_name = weight_variable_name_solution, \n",
    "    sample_values_dict = sample_values_dict, \n",
    "    select_top_percent = 100, \n",
    "    n_bands = 10, \n",
    "    rows = 10, \n",
    "    data_path = data_path\n",
    "    )\n",
    "rf_optuna_eval = rf_optuna_report_class.get_evaluation(predicted_score_binary = 'rf_score_optuna_binary',\n",
    "                                                            filename = 'evaluation_metrics_RF_optuna.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac7d286-3c4d-4ace-a939-04ab8c7f7f01",
   "metadata": {},
   "source": [
    "## Calculate RF Lifting table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b20d64-5f18-4776-b0a9-4e886e0cf234",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_report_class = rf_optuna_report_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb5e56d-3e0a-42a6-8f29-977c468f19b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Lift table\n",
    "binary_report_lift_table = binary_report_class.create_lift_table(filename = 'lift_table_RF_')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbd8dcb-b477-4898-b8ef-57dba0b80e98",
   "metadata": {},
   "source": [
    "## Plots RF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12daa74-69ba-4f03-8a6f-7b86a40634f4",
   "metadata": {},
   "source": [
    "### Plot RF Detection rate vs. Population Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4828068-57ad-42d3-9d21-0a0a39408d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder, if it doesn't exist\n",
    "folder_name = 'graphs_RF'\n",
    "ufun.create_folder(data_path = data_path, \n",
    "                   folder_name = 'output/{}'.format(folder_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c9d265-1f28-49db-885c-a1bed82f999a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "binary_report_class.plot_ADR_Quantile(\n",
    "        folder_name = folder_name,\n",
    "        xlim=None, \n",
    "        ylim=None\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79d4e37-2a3a-4a1b-aec6-52dc2db73a5a",
   "metadata": {},
   "source": [
    "### Plot RF Cum. Detection rate vs. Population Distribution (Gains chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6637ca04-67d0-4da7-848f-3001463e639d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "binary_report_class.plot_cADR_Quantile(\n",
    "        folder_name = folder_name,\n",
    "        xlim=None, \n",
    "        ylim=None\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75badd6f-e4fc-49e5-b106-64b269ad9674",
   "metadata": {},
   "source": [
    "### Plot RF FPR vs. Population Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2c04c1-273f-4717-b778-935c916826e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "binary_report_class.plot_FPR_Quantile(\n",
    "        folder_name = folder_name,\n",
    "        xlim=None, \n",
    "        ylim=None\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3011f5e-3e71-4f1d-8247-d29781ea33fd",
   "metadata": {},
   "source": [
    "### Plot RF Cum. FPR vs. Population Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7639e6-1816-423a-96e3-ecaa28142aa9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "binary_report_class.plot_cFPR_Quantile(\n",
    "        folder_name = folder_name,\n",
    "        xlim=None, \n",
    "        ylim=None\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863e2d96-c390-463c-b357-5f67e1b83b2a",
   "metadata": {},
   "source": [
    "### Plot RF ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e087e6-76b3-4e00-84ef-b7b504a6e249",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "binary_report_class.plot_ROC_curve(folder_name = folder_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc61ee8-b0db-4b55-9c7f-ca5a5abc80cc",
   "metadata": {},
   "source": [
    "### Plot RF Precision Recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b8fe4a-07e0-4072-9664-271b7247cff1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "binary_report_class.plot_precision_recall_curve(folder_name = folder_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ccdac1-3c9a-434f-a44c-e46bca02e337",
   "metadata": {},
   "source": [
    "### Plot RF F1 score, Accuracy, Sensitivity, Specificity, Precision vs Cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792bd45c-24a4-4fae-90fb-0183016d976e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "binary_report_class.plot_cutoffs(\n",
    "        folder_name = folder_name,\n",
    "        n_bands = 100, # Number of bands between 0 and 1\n",
    "        return_table=False # Set to True in order to return the table that produced the graph, otherwise set to False\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f3f09f-73d2-4446-ba99-f197fb2cfb57",
   "metadata": {},
   "source": [
    "# Random Forest with H2O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a34cc9-9311-4243-b7b4-25f2844f60ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "import h2o \n",
    "h2o.init(nthreads=8, max_mem_size=12)\n",
    "\n",
    "data_h2o = data.copy()\n",
    "for k in data.keys():\n",
    "    # load pandas dataframe into H2O dataframe\n",
    "    data_h2o[k] = h2o.H2OFrame(data[k])\n",
    "    # Define response variable to be categorical for classification model\n",
    "    data_h2o[k][target_variable_name] = data_h2o[k][target_variable_name].asfactor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fc6296-1e18-4b59-b3df-5127eb3f0e19",
   "metadata": {},
   "source": [
    "## Feature selection RF H2O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f744ff-b2fd-4497-81f9-46c3326a38ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide input data to feature selection class\n",
    "select = fe.SelectBest_weight(df=data['data_{}'.format(sample_values_solution[0])], \n",
    "                              target=target_variable_name, \n",
    "                              weight=weight_variable_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28affa75-0ad1-4fea-96da-f0990cf0ec4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model_rf = RandomForestClassifier(n_estimators=200, max_depth=5, random_state=1234, n_jobs=6)\n",
    "rf_importance, feats_best_rf = select.top_rf_feat(feats=keep_num_vars_gini, model=model_rf, n=rf_initial_screening_var_threshold)\n",
    "print('The top Random Forest features are: \\n', feats_best_rf)\n",
    "rf_importance\t\t\t\t\t\t\t  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11107dc-213c-4a45-93f2-19e4488a364a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_best_rf_back = select.backward_recur(feats=feats_best_rf, \n",
    "                                           oos=data['data_{}'.format(sample_values_solution[1])], \n",
    "                                           model=model_rf, \n",
    "                                           min_feats=rf_backward_selection_var_threshold, \n",
    "                                           classification=True)\n",
    "print(feats_best_rf_back)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e03bc35-2de1-4fe1-960b-bb70d7b57252",
   "metadata": {},
   "source": [
    "## Grid search and train RF machine learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5266861-4acc-488c-baaf-34db933a7bc9",
   "metadata": {},
   "source": [
    "### Grid search based on iterative step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a185a24-3e2c-41c1-9236-ab31232a4d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning: The process works as follows:\n",
    "# 1) Iterate n_estimators (grid) for the fixed values of max_depth and max_features (params). Select the value of n_estimators that minimizes the loss function. \n",
    "# 2) Keep the n_estimators value from the previous step. Iterate max_depth (grid) for the fixed value of max_features (params). Select the value of max_depth that minimizes the loss function. \n",
    "# 3) Keep the max_depth value from the previous step. Iterate max_features (grid). Select the value of max_features that minimizes the loss function. \n",
    "import math\n",
    "# 18 models in total\n",
    "grid = {'n_estimators':[10, 50, 80, 100, 200, 400],  # Number of trees, 100-500 is usually sufficient /\n",
    "       'max_depth':[2, 5, 10, 20],  # Depth of the tree, 5-6 is usually default\n",
    "       'max_features':[*range(1,len(feats_best_rf_back),math.ceil(len(feats_best_rf_back)/5))],  # Number of variables randomly sampled as candidates at each split, sqrt(p) is usually the default\n",
    "       'min_samples_leaf': [1, 10, 100]} # Minimum number of observations in each leaf, 10 is usually default\n",
    "\n",
    "params = {'n_estimators': 100, 'max_depth':10, 'max_features':math.ceil(math.sqrt(len(feats_best_rf_back))), 'min_samples_leaf':10}\n",
    "\n",
    "opt_params_iterative, loss = mb.step_search_weight(estimator=RandomForestClassifier, params=params, grid=grid, target=target_variable_name, \n",
    "                                  weight=weight_variable_name, dev=data['data_{}'.format(sample_values_solution[0])], val=data['data_{}'.format(sample_values_solution[1])], keep=feats_best_rf_back)\n",
    "print('\\n Best Parameters')\n",
    "print(opt_params_iterative)\n",
    "print(loss)\n",
    "\n",
    "# Load RF machine leanring library\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "\n",
    "# Define the random forest model\n",
    "rf = RandomForestClassifier(n_estimators=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='n_estimators'][0], \n",
    "                            max_depth=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='max_depth'][0], \n",
    "                            max_features=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='max_features'][0], \n",
    "                            min_samples_leaf=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='min_samples_leaf'][0], \n",
    "                            random_state=0)\n",
    "\n",
    "# Train the model\n",
    "rf_model = mb.fit_model_weight(df=data['data_{}'.format(sample_values_solution[0])], \n",
    "                               feats=feats_best_rf_back, \n",
    "                               target=target_variable_name, \n",
    "                               weight=weight_variable_name, \n",
    "                               model=rf, \n",
    "                               model_name=data_path + '/output/' + 'rf_h2o_iterative.pkl')\n",
    "\n",
    "# Compute the score for the OOT data\n",
    "for k in data.keys():\n",
    "    data[k].loc[:, 'rf_score_iterative_numeric'] = rf_model.predict_proba(data[k].loc[:, feats_best_rf_back].values)[:, 1]\n",
    "    data[k].loc[:, 'rf_score_iterative_binary'] = list(map(round, data[k].loc[:, 'rf_score_iterative_numeric']))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894beccb-e55b-4563-8b1f-10176d7e36b9",
   "metadata": {},
   "source": [
    "### Grid search based on H2O random grid space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6750128d-5480-4683-9d59-f95490166b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "start = timeit.default_timer()\n",
    "\n",
    "# Hyperparameter tuning using H2O random grid space: - NOTE: THIS TAKES 5 MINUTES TO RUN!\n",
    "from h2o.grid.grid_search import H2OGridSearch\n",
    "from h2o.estimators import H2ORandomForestEstimator\n",
    "import math\n",
    "\n",
    "# RF hyperparameters\n",
    "# 1,440 models in total\n",
    "grid_h2o_random = {'ntrees': [i*50 for i in range(1, 10)], # Number of trees, 100-500 is usually sufficient\n",
    "                'max_depth': list(range(2, 20, 3)), # Depth of the tree, 5-6 is usually default\n",
    "                'mtries': [i*2 for i in range(1, math.ceil(len(feats_best_rf_back)/2))], # Number of variables randomly sampled as candidates at each split, sqrt(p) is usually the default\n",
    "                'min_rows': [1, 10, 100], # Minimum number of observations in each leaf, 10 is usually default\n",
    "                'nbins': [20, 30], #For numerical columns (real/int), build a histogram of (at least) this many bins, then split at the best point, 30 is usually default\n",
    "                'nbins_cats': [10, 15], # For categorical columns (factors), build a histogram of this many bins, then split at the best point. Higher values can lead to more overfitting, 10-20 is usally default\n",
    "                'sample_rate': [0.63], # Row sample rate per tree (from 0.0 to 1.0), 0.63 is usually default\n",
    "                'col_sample_rate_per_tree': [1]} # Column sample rate per tree (from 0.0 to 1.0), 1 is usually default\n",
    "\n",
    "# Search criteria\n",
    "search_criteria = {'strategy': 'RandomDiscrete', 'max_models': 10, 'seed': 1}\n",
    "\n",
    "# Train and validate a random grid of RFs\n",
    "rf_h2o_grid_random = H2OGridSearch(model=H2ORandomForestEstimator,\n",
    "                          grid_id='rf_h2o_grid_random',\n",
    "                          hyper_params=grid_h2o_random,\n",
    "                          search_criteria=search_criteria)\n",
    "\n",
    "rf_h2o_grid_random.train(x=feats_best_rf_back, \n",
    "                         y=target_variable_name, \n",
    "                         training_frame=data_h2o['data_{}'.format(sample_values_solution[0])], \n",
    "#                         validation_frame=data_h2o['data_{}'.format(sample_values_solution[1])], \n",
    "                         weights_column=weight_variable_name, \n",
    "                         seed=1, \n",
    "                         nfolds=2)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Execution time in seconds: ', stop - start)  \n",
    "# Execution time in seconds:  53.19511840000001\n",
    "\n",
    "# Get the grid results, sorted by validation logloss\n",
    "rf_h2o_grid_random_perf = rf_h2o_grid_random.get_grid(sort_by='logloss', decreasing=False)\n",
    "print(rf_h2o_grid_random_perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad51a26-17df-40e2-aeab-bea5a6283e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build random forest model in H2O, based on the H2O random grid search. Grab the best model.\n",
    "rf_h2o_random = rf_h2o_grid_random_perf.models[0]\n",
    "\n",
    "# Check the random forest model performance on the OOT sample\n",
    "rf_h2o_model_performance = rf_h2o_random.model_performance(data_h2o['data_{}'.format(sample_values_solution[1])])\n",
    "print(rf_h2o_model_performance)\n",
    "print('Gini for the OOT sample is: ', 2*rf_h2o_model_performance.auc()-1)\n",
    "\n",
    "# Check the variable importance \n",
    "var_imp=[(x[0],x[2]) for x in rf_h2o_random.varimp()]\n",
    "for v1, v2 in var_imp:\n",
    "    print (v1,v2)\n",
    "\n",
    "# Compute the score for the OOT data\n",
    "for k in data_h2o.keys():\n",
    "    data_h2o[k]['rf_h2o_score_random_numeric'] = rf_h2o_random.predict(data_h2o[k])[2]\n",
    "    data_h2o[k]['rf_h2o_score_random_binary'] = data_h2o[k]['rf_h2o_score_random_numeric'].round(digits=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467aafd0-7fa4-4cc4-af55-6df14b0faf41",
   "metadata": {},
   "source": [
    "### Grid search based on H2O fixed grid space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca25be4-e3e7-4d13-bf29-b89a1ae3a081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning using H2O fixed grid space: - NOTE: THIS TAKES SEVERAL MINUTES TO RUN!\n",
    "import timeit\n",
    "start = timeit.default_timer()\n",
    "\n",
    "from h2o.grid.grid_search import H2OGridSearch\n",
    "from h2o.estimators import H2ORandomForestEstimator\n",
    "import math\n",
    "\n",
    "# 27 models in total\n",
    "grid_h2o_fixed = {'ntrees':[50, 100, 200],  # Number of trees, 100-500 is usually sufficient\n",
    "       'max_depth':[8, 10, 12],  # Depth of the tree, 5-6 is usually default\n",
    "       'mtries':[8, 10, 12],  # Number of variables randomly sampled as candidates at each split, sqrt(p) is usually the default\n",
    "        'min_rows': [10],  # Minimum number of observations in each leaf, 10 is usually default\n",
    "        'nbins': [30],  #For numerical columns (real/int), build a histogram of (at least) this many bins, then split at the best point, 30 is usually default\n",
    "        'nbins_cats': [15],  # For categorical columns (factors), build a histogram of this many bins, then split at the best point. Higher values can lead to more overfitting, 10-20 is usally default\n",
    "        'sample_rate': [0.63],  # Row sample rate per tree (from 0.0 to 1.0), 0.63 is usually default\n",
    "        'col_sample_rate_per_tree': [1]} # Column sample rate per tree (from 0.0 to 1.0), 1 is usually default\n",
    "\n",
    "# Train and validate a cartesian grid of RFs\n",
    "rf_h2o_grid_fixed = H2OGridSearch(model=H2ORandomForestEstimator,\n",
    "                          grid_id='rf_h2o_grid_fixed',\n",
    "                          hyper_params=grid_h2o_fixed)\n",
    "\n",
    "rf_h2o_grid_fixed.train(x=feats_best_rf_back, \n",
    "                        y=target_variable_name, \n",
    "                        training_frame=data_h2o['data_{}'.format(sample_values_solution[0])], \n",
    "#                        validation_frame=data_h2o['data_{}'.format(sample_values_solution[1])], \n",
    "                        weights_column=weight_variable_name, \n",
    "                        seed=1, \n",
    "                        nfolds=2)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Execution time in seconds: ', stop - start)  \n",
    "# Execution time in seconds:  93.57661189999999\n",
    "\n",
    "# Get the grid results, sorted by validation logloss\n",
    "rf_h2o_grid_fixed_perf = rf_h2o_grid_fixed.get_grid(sort_by='logloss', decreasing=False)\n",
    "print(rf_h2o_grid_fixed_perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ab7f22-9014-4b9e-bae6-8a004420862d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build fixed forest model in H2O, based on the H2O fixed grid search. Grab the best model.\n",
    "rf_h2o_fixed = rf_h2o_grid_fixed_perf.models[0]\n",
    "\n",
    "# Check the random forest model performance on the OOT sample\n",
    "rf_h2o_model_performance = rf_h2o_fixed.model_performance(data_h2o['data_{}'.format(sample_values_solution[1])])\n",
    "print(rf_h2o_model_performance)\n",
    "print('Gini for the OOT sample is: ', 2*rf_h2o_model_performance.auc()-1)\n",
    "\n",
    "# Check the variable importance \n",
    "var_imp=[(x[0],x[2]) for x in rf_h2o_fixed.varimp()]\n",
    "for v1, v2 in var_imp:\n",
    "    print (v1,v2)\n",
    "\n",
    "# Compute the score for the OOT data\n",
    "for k in data_h2o.keys():\n",
    "    data_h2o[k]['rf_h2o_score_fixed_numeric'] = rf_h2o_fixed.predict(data_h2o[k])[2]\n",
    "    data_h2o[k]['rf_h2o_score_fixed_binary'] = data_h2o[k]['rf_h2o_score_fixed_numeric'].round(digits=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75127647-b1e7-4cee-862a-2cdc91ccbb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert H2O dataframe to pandas dataframe\n",
    "for k in data_h2o.keys():\n",
    "    data[k]=data_h2o[k].as_data_frame(use_pandas=True, use_multi_thread=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e005151-be86-4618-b707-568ffdf94bae",
   "metadata": {},
   "source": [
    "## Produce H2O RF reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a86d35-7ce1-4531-a195-1e308c410d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_h2o_random_report_class = rp.binary_regression_report(\n",
    "    predictions_dictionary = data, \n",
    "    target_variable = target_variable_name, \n",
    "    predicted_score_numeric = 'rf_h2o_score_random_numeric', \n",
    "    amount_variable_name = amount_variable_name_solution, \n",
    "    weight_variable_name = weight_variable_name_solution, \n",
    "    sample_values_dict = sample_values_dict, \n",
    "    select_top_percent = 100, \n",
    "    n_bands = 10, \n",
    "    rows = 10, \n",
    "    data_path = data_path\n",
    "    )\n",
    "rf_h2o_random_eval = rf_h2o_random_report_class.get_evaluation(predicted_score_binary = 'rf_h2o_score_random_binary', \n",
    "                                                       filename = 'evaluation_metrics_RF_H2O_random.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b6d813-9238-448f-a35d-2c612884c7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_h2o_fixed_report_class = rp.binary_regression_report(\n",
    "    predictions_dictionary = data, \n",
    "    target_variable = target_variable_name, \n",
    "    predicted_score_numeric = 'rf_h2o_score_fixed_numeric', \n",
    "    amount_variable_name = amount_variable_name_solution, \n",
    "    weight_variable_name = weight_variable_name_solution, \n",
    "    sample_values_dict = sample_values_dict, \n",
    "    select_top_percent = 100, \n",
    "    n_bands = 10, \n",
    "    rows = 10, \n",
    "    data_path = data_path\n",
    "    )\n",
    "rf_h2o_fixed_eval = rf_h2o_fixed_report_class.get_evaluation(predicted_score_binary = 'rf_h2o_score_fixed_binary', \n",
    "                                                       filename = 'evaluation_metrics_RF_H2O_fixed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c3f2e0-558b-4143-bc3b-bd6a465a1a1c",
   "metadata": {},
   "source": [
    "## Calculate H2O RF Lifting table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a7ff50-b525-4ae0-98b3-040295b7618e",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_report_class_h2o = rf_h2o_random_report_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9cff06-2bd2-4125-bcaa-579d56df9044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Lift table\n",
    "binary_report_lift_table_h2o = binary_report_class_h2o.create_lift_table(filename = 'lift_table_RF_H2O_')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98a0b6b-0217-4327-b05f-efbf1aeb7c22",
   "metadata": {},
   "source": [
    "## Plots RF H2O"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28031b74-f85f-4525-bac4-95d49db9ff06",
   "metadata": {},
   "source": [
    "### Plot RF H2O Detection rate vs. Population Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5479f1f5-7e74-4c76-9096-c14fae876ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder, if it doesn't exist\n",
    "folder_name = 'graphs_RF_H2O'\n",
    "ufun.create_folder(data_path = data_path, \n",
    "                   folder_name = 'output/{}'.format(folder_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaad3257-7bad-430c-9729-79183025ffc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_report_class_h2o.plot_ADR_Quantile(\n",
    "        folder_name = folder_name,\n",
    "        xlim=None, \n",
    "        ylim=None\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb105228-0457-44bc-8e01-b3d39c26979d",
   "metadata": {},
   "source": [
    "### Plot RF H2O Cum. Detection rate vs. Population Distribution (Gains chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bd511c-978a-4646-ae7d-18eb62773dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_report_class_h2o.plot_cADR_Quantile(\n",
    "        folder_name = folder_name,\n",
    "        xlim=None, \n",
    "        ylim=None\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba7eb98-c9cd-474a-99d8-ef9ba6a3de19",
   "metadata": {},
   "source": [
    "### Plot RF H2O FPR vs. Population Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de786cc4-5978-422e-ad55-bd146c950786",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_report_class_h2o.plot_FPR_Quantile(\n",
    "        folder_name = folder_name,\n",
    "        xlim=None, \n",
    "        ylim=None\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562941e9-a8d9-442c-ad2d-f95628b2a052",
   "metadata": {},
   "source": [
    "### Plot RF H2O Cum. FPR vs. Population Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceca0341-8914-4147-b0fa-5845a65fc82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_report_class_h2o.plot_cFPR_Quantile(\n",
    "        folder_name = folder_name,\n",
    "        xlim=None, \n",
    "        ylim=None\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba8f72a-8cb1-4298-a31c-26815475eee8",
   "metadata": {},
   "source": [
    "### Plot RF H2O ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8594d6ee-559e-4dce-ae16-7040f4a425ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_report_class_h2o.plot_ROC_curve(folder_name = folder_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf0b0b7-79ac-4c14-a2ef-dad9ba564e24",
   "metadata": {},
   "source": [
    "### Plot RF H2O Precision Recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a5f334-88ff-4fd8-a8a4-e5e8f5d8717f",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_report_class_h2o.plot_precision_recall_curve(folder_name = folder_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97883d6-e22b-478b-91ea-cee659e1db61",
   "metadata": {},
   "source": [
    "### Plot RF H2O F1 score, Accuracy, Sensitivity, Specificity, Precision vs Cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29bc87a-efca-4c21-85f9-f75827dae8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_report_class_h2o.plot_cutoffs(\n",
    "        folder_name = folder_name,\n",
    "        n_bands = 100, # Number of bands between 0 and 1\n",
    "        return_table=False # Set to True in order to return the table that produced the graph, otherwise set to False\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba15b66-c5fc-40fd-9141-35b88a2fd7f0",
   "metadata": {},
   "source": [
    "## Save model and export MOJO file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f557d841-7b0f-4668-90ea-7f5bd445913d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_h2o_model_to_save = rf_h2o_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de558a7e-379c-4b6f-b11c-4b8842965f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save already trained model for future use\n",
    "model_path=h2o.save_model(model=rf_h2o_model_to_save, path='{}/output/rf_h2o_grid_model'.format(data_path), force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef2db00-7ad7-41ad-b2c8-50058fabdb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save mojo file, which helps deploy the train model\n",
    "rf_h2o_model_to_save.download_mojo(path='{}/output/model.zip'.format(data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dcf95b-9534-4836-99f3-0aca9b7f5c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o.cluster().shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4ac653-570c-4919-887d-b94ca05942cb",
   "metadata": {},
   "source": [
    "# Gradient Boosting Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e832c5-0416-4f53-ae58-94187deea240",
   "metadata": {},
   "source": [
    "## Feature selection GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4108e4d-50da-4e5c-a16a-45cdae9feb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide input data to feature selection class\n",
    "select = fe.SelectBest_weight(df=data['data_{}'.format(sample_values_solution[0])], \n",
    "                              target=target_variable_name, \n",
    "                              weight=weight_variable_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1207975-d2a2-4cde-8889-e8ea6180bdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top features to apply a first screening\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "model_gbm = GradientBoostingClassifier(n_estimators=200, max_depth=5, learning_rate=0.1, random_state=1234)\n",
    "gbm_importance, feats_best_gbm = select.top_gbm_feat(feats=keep_num_vars_gini, model=model_gbm, n=15)\n",
    "print('The top GBM features are: \\n', feats_best_gbm)\n",
    "gbm_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc70e6c5-1e4a-40a0-9c3f-b6e100722d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward selection with gradient boosting machine model: In each step a feature is selected to remove - this removal maximizes Gini\n",
    "feats_best_gbm_back = select.backward_recur(feats=feats_best_gbm, \n",
    "                                            oos=data['data_{}'.format(sample_values_solution[1])], \n",
    "                                            model=model_gbm, \n",
    "                                            min_feats=10, \n",
    "                                            classification=True)\n",
    "print(feats_best_gbm_back)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc18065f-27fc-4eed-aa8c-12c8b0e42974",
   "metadata": {},
   "source": [
    "## Grid search and train GBM machine learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221a0189-e0ae-4fc8-bc77-282195bf92f2",
   "metadata": {},
   "source": [
    "### Grid search based on iterative step search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bc3810-68ee-4ba3-83ac-4c31c069172e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameter tuning: The process works as follows:\n",
    "# 1) Iterate n_estimators (grid) for the fixed values of max_depth, max_features, and learning_rate (params). Select the value of n_estimators that minimizes the loss function. \n",
    "# 2) Keep the n_estimators value from the previous step. Iterate max_depth (grid) for the fixed value of max_features, learning_rate (params). Select the value of max_depth that minimizes the loss function. \n",
    "# 3) Keep the max_depth value from the previous step. Iterate max_features (grid) for the fixed value of learning_rate (params). Select the value of max_features that minimizes the loss function. \n",
    "# 4) Keep the max_features value from the previous step. Iterate learning_rate (grid). Select the value of learning_rate that minimizes the loss function. \n",
    "# 40 models in total\n",
    "import numpy as np\n",
    "\n",
    "grid = {'n_estimators':[10, 20, 50, 100, 200], # Number of trees, 100-500 is usually sufficient\n",
    "       'max_depth':[2, 5, 10], # Depth of the tree, 5-6 is usually default\n",
    "        \"max_features\":[\"log2\",\"sqrt\"], # Number of variables randomly sampled as candidates at each split, sqrt(p) is usually the default\n",
    "#       'max_features':[*range(1,len(feats_best_gbm_back),math.ceil(len(feats_best_gbm_back)/5))], \\\n",
    "       'learning_rate':[0.05, 0.1, 0.2, 0.5], # Learing rate. Shrinkage is used for reducing, or shrinking, the impact of each additional fitted \n",
    "# base-learner (tree). It reduces the size of incremental steps and thus penalizes the importance of each consecutive iteration. \n",
    "# The intuition behind this technique is that it is better to improve a model by taking many small steps than by taking fewer large steps. \n",
    "# If one of the boosting iterations turns out to be erroneous, its negative impact can be easily corrected in subsequent steps. \n",
    "# High learn rates and especially values close to 1.0 typically result in overfit models with poor performance. \n",
    "# Values much smaller than .01 significantly slow down the learning process and might be reserved for overnight runs. 0.10 default\n",
    "        \"min_samples_leaf\": np.linspace(0.01, 0.4, 10), # Minimum number of observations in each leaf, 10 is usually default\n",
    "        \"min_samples_split\": np.linspace(0.01, 0.4, 10), # The minimum number of observations required to split an internal node\n",
    "        # If int then min_samples_split is the minimum number of observations\n",
    "        # If float then then minimum number of observations is a fraction and ceil(min_samples_split * n_samples) \n",
    "        \"subsample\":[0.5, 0.7, 0.8, 0.9, 0.95, 1.0], # The fraction of samples from n_estimators\n",
    "        \"criterion\": [\"friedman_mse\",  \"squared_error\"] # The function to measure the quality of a split\n",
    "#        , \"loss\":[\"deviance\"]      \n",
    "       }\n",
    "\n",
    "params = {'n_estimators': 100, 'max_depth':4, 'max_features':\"sqrt\", 'learning_rate':0.1, \"min_samples_leaf\":0.2, \"min_samples_split\": 0.2, \\\n",
    "         \"subsample\":0.8, \"criterion\":\"friedman_mse\"\n",
    "         # , \"loss\":[\"deviance\"]\n",
    "         }            \n",
    "\n",
    "opt_params_iterative, loss = mb.step_search_weight(estimator=GradientBoostingClassifier, \n",
    "                                                   params=params, \n",
    "                                                   grid=grid, \n",
    "                                                   target=target_variable_name, \n",
    "                                                   weight=weight_variable_name, \n",
    "                                                   dev=data['data_{}'.format(sample_values_solution[0])], \n",
    "                                                   val=data['data_{}'.format(sample_values_solution[1])], \n",
    "                                                   keep=feats_best_gbm_back)\n",
    "print('\\n Best Parameters')\n",
    "print(opt_params_iterative)\n",
    "print(loss)\n",
    "\n",
    "# Load GBM algorithm\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Define the GBM model\n",
    "gbm = GradientBoostingClassifier(n_estimators=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='n_estimators'][0], \n",
    "                                 max_depth=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='max_depth'][0], \n",
    "                                 max_features=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='max_features'][0], \n",
    "                                 learning_rate=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='learning_rate'][0], \n",
    "                                 min_samples_leaf=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='min_samples_leaf'][0], \n",
    "                                 min_samples_split=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='min_samples_split'][0], \n",
    "                                 subsample=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='subsample'][0], \n",
    "                                 criterion=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='criterion'][0],                                 \n",
    "                                 random_state=0)\n",
    "\n",
    "# Train the model\n",
    "gbm_model = mb.fit_model_weight(data['data_{}'.format(sample_values_solution[0])], \n",
    "                                feats_best_gbm_back, \n",
    "                                target_variable_name, \n",
    "                                weight_variable_name, \n",
    "                                gbm, \n",
    "                                data_path + '/output/' + 'gbm_iterative.pkl')\n",
    "\n",
    "# Compute the score for the OOT data\n",
    "# Compute the score for the OOT data\n",
    "for k in data.keys():\n",
    "    data[k].loc[:, 'gbm_score_iterative_numeric'] = gbm_model.predict_proba(data[k].loc[:, feats_best_gbm_back].values)[:, 1]\n",
    "    data[k].loc[:, 'gbm_score_iterative_binary'] = list(map(round, data[k].loc[:, 'gbm_score_iterative_numeric']))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fe4ab8-7ac9-46f1-a78f-86dc1f8a9607",
   "metadata": {},
   "source": [
    "### Grid search based on RandomizedSearchCV - also uses cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eeb7d2-c286-44ae-8258-8a734789f236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 720 models in total\n",
    "grid = {'n_estimators':[100, 200, 300, 400, 500], # Number of trees, 100-500 is usually sufficient\n",
    "       'max_depth':[1, 2, 4, 6], # Depth of the tree, 5-6 is usually default\n",
    "        \"max_features\":[\"sqrt\", 'log2'], # Number of variables randomly sampled as candidates at each split, sqrt(p) is usually the default\n",
    "#       'max_features':[*range(1,len(feats_best_gbm_back),math.ceil(len(feats_best_gbm_back)/5))], \\\n",
    "       'learning_rate':[0.1, 0.2, 0.3], # Learing rate. Shrinkage is used for reducing, or shrinking, the impact of each additional fitted \n",
    "# base-learner (tree). It reduces the size of incremental steps and thus penalizes the importance of each consecutive iteration. \n",
    "# The intuition behind this technique is that it is better to improve a model by taking many small steps than by taking fewer large steps. \n",
    "# If one of the boosting iterations turns out to be erroneous, its negative impact can be easily corrected in subsequent steps. \n",
    "# High learn rates and especially values close to 1.0 typically result in overfit models with poor performance. \n",
    "# Values much smaller than .01 significantly slow down the learning process and might be reserved for overnight runs. 0.10 default\n",
    "        \"min_samples_leaf\": [0.01, 0.05, 0.10, 0.15], # Minimum number of observations in each leaf, 10 is usually default\n",
    "        \"min_samples_split\": [0.1, 0.2, 0.3], # The minimum number of observations required to split an internal node\n",
    "        # If int then min_samples_split is the minimum number of observations\n",
    "        # If float then then minimum number of observations is a fraction and ceil(min_samples_split * n_samples) \n",
    "        \"subsample\":[0.8], # The fraction of samples from n_estimators\n",
    "        \"criterion\": [\"friedman_mse\"] # The function to measure the quality of a split\n",
    "#        , \"loss\":[\"deviance\"]      \n",
    "       }\n",
    "\n",
    "gbm_grid_random_search_model = mb.grid_search_cv(\n",
    "    n_splits = 2, # Number of cross-validation splits\n",
    "    classifier = GradientBoostingClassifier, # Classifier name, e.g. RandomForestClassifier\n",
    "    keras_function = None, # Define Keras function. If Keras is not used, then leave this parameter blank\n",
    "    grid_params = grid, # Grid space\n",
    "    dev_df = data['data_{}'.format(sample_values_solution[0])],  # Development sample that this will analysis will be performed\n",
    "    feats = feats_best_gbm_back, # List of predictor names\n",
    "    target = target_variable_name, # Target variable name\n",
    "    weight_variable = weight_variable_name, # Weight variable name\n",
    "    randomized_search = True, # Set to True if randomized grid search will be performed, or to False if exhaustive grid search will be performed\n",
    "    n_random_grids = 100, # Number of grid searches when randomized_search=True. If randomized_search=False, then this parameter is not applicable\n",
    "    random_state = None, # If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.\n",
    "    n_jobs = 8 # Number of jobs to run in parallel\n",
    ")\n",
    "\n",
    "# Save the best parameters\n",
    "opt_params_random = gbm_grid_random_search_model.best_params_\n",
    "print('The best hyperparameter set is:', opt_params_random)\n",
    "print('Mean loss function for cross-validation test data: ', -gbm_grid_random_search_model.cv_results_['mean_test_score'][gbm_grid_random_search_model.best_index_])\n",
    "print('Standard deviation loss function for cross-validation test data: ', gbm_grid_random_search_model.cv_results_['std_test_score'][gbm_grid_random_search_model.best_index_])\n",
    "#print('Mean Gini for OOT data', 2*gbm_grid_random_search_model.score(oos[feats_best_gbm_back], oos[target_variable_name], sample_weight=oos[weight_variable_name])-1)\n",
    "\n",
    "rp.plot_cross_validation_score(model=gbm_grid_random_search_model)\n",
    "\n",
    "# Compute the score for the OOT data\n",
    "for k in data.keys():\n",
    "    data[k].loc[:, 'gbm_score_random_numeric'] = rf_grid_random_search_model.predict_proba(data[k][feats_best_rf_back].values)[:, 1]\n",
    "    data[k].loc[:, 'gbm_score_random_binary'] = list(map(round, data[k].loc[:, 'gbm_score_random_numeric']))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91710b6-110a-4f92-896a-e70d5391f8e5",
   "metadata": {},
   "source": [
    "### Grid search based on GridSearchCV - also uses cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1321aa2f-211f-4207-84f1-fc0993c4b29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 81 models in total\n",
    "grid = {'n_estimators':[100, 200, 300], # Number of trees, 100-500 is usually sufficient\n",
    "       'max_depth':[2, 4, 6], # Depth of the tree, 5-6 is usually default\n",
    "        \"max_features\":[\"sqrt\"], # Number of variables randomly sampled as candidates at each split, sqrt(p) is usually the default\n",
    "#       'max_features':[*range(1,len(feats_best_gbm_back),math.ceil(len(feats_best_gbm_back)/5))], \\\n",
    "       'learning_rate':[0.1, 0.2, 0.3], # Learing rate. Shrinkage is used for reducing, or shrinking, the impact of each additional fitted \n",
    "# base-learner (tree). It reduces the size of incremental steps and thus penalizes the importance of each consecutive iteration. \n",
    "# The intuition behind this technique is that it is better to improve a model by taking many small steps than by taking fewer large steps. \n",
    "# If one of the boosting iterations turns out to be erroneous, its negative impact can be easily corrected in subsequent steps. \n",
    "# High learn rates and especially values close to 1.0 typically result in overfit models with poor performance. \n",
    "# Values much smaller than .01 significantly slow down the learning process and might be reserved for overnight runs. 0.10 default\n",
    "        \"min_samples_leaf\": [0.05, 0.10, 0.15], # Minimum number of observations in each leaf, 10 is usually default\n",
    "        \"min_samples_split\": [0.2], # The minimum number of observations required to split an internal node\n",
    "        # If int then min_samples_split is the minimum number of observations\n",
    "        # If float then then minimum number of observations is a fraction and ceil(min_samples_split * n_samples) \n",
    "        \"subsample\":[0.8], # The fraction of samples from n_estimators\n",
    "        \"criterion\": [\"friedman_mse\"] # The function to measure the quality of a split\n",
    "#        , \"loss\":[\"deviance\"]      \n",
    "       }\n",
    "\n",
    "gbm_grid_fixed_search_model = mb.grid_search_cv(\n",
    "    n_splits = 2, # Number of cross-validation splits\n",
    "    classifier = GradientBoostingClassifier, # Classifier name, e.g. RandomForestClassifier\n",
    "    keras_function = None, # Define Keras function. If Keras is not used, then leave this parameter blank\n",
    "    grid_params = grid, # Grid space\n",
    "    dev_df = data['data_{}'.format(sample_values_solution[0])],  # Development sample that this will analysis will be performed\n",
    "    feats = feats_best_gbm_back, # List of predictor names\n",
    "    target = target_variable_name, # Target variable name\n",
    "    weight_variable = weight_variable_name, # Weight variable name\n",
    "    randomized_search = False, # Set to True if randomized grid search will be performed, or to False if exhaustive grid search will be performed\n",
    "    n_random_grids = 1, # Number of grid searches when randomized_search=True. If randomized_search=False, then this parameter is not applicable\n",
    "    random_state = None, # If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.\n",
    "    n_jobs = 8 # Number of jobs to run in parallel\n",
    ")\n",
    "\n",
    "# Save the best parameters\n",
    "opt_params_fixed = gbm_grid_fixed_search_model.best_params_\n",
    "print('The best hyperparameter set is:', opt_params_fixed)\n",
    "print('Mean loss function for cross-validation test data: ', -gbm_grid_fixed_search_model.cv_results_['mean_test_score'][gbm_grid_fixed_search_model.best_index_])\n",
    "print('Standard deviation loss function for cross-validation test data: ', gbm_grid_fixed_search_model.cv_results_['std_test_score'][gbm_grid_fixed_search_model.best_index_])\n",
    "#print('Mean Gini for OOT data', 2*gbm_grid_fixed_search_model.score(oos[feats_best_gbm_back], oos[target_variable_name], sample_weight=oos[weight_variable_name])-1)\n",
    "\n",
    "rp.plot_cross_validation_score(model=gbm_grid_fixed_search_model)\n",
    "\n",
    "# Compute the score for the OOT data\n",
    "for k in data.keys():\n",
    "    data[k].loc[:, 'gbm_score_fixed_numeric'] = rf_grid_fixed_search_model.predict_proba(data[k][feats_best_rf_back].values)[:, 1]\n",
    "    data[k].loc[:, 'gbm_score_fixed_binary'] = list(map(round, data[k].loc[:, 'gbm_score_fixed_numeric']))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce4c76e-e2dc-4836-85ed-8804dfb04265",
   "metadata": {},
   "source": [
    "### Grid search based on Optuna - also uses cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70988473-780d-4030-a20a-cb23da3f7b3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gbm_optuna_search_model = mb.grid_search_optuna(\n",
    "    classifier = GradientBoostingClassifier, # Classifier name, e.g. RandomForestClassifier\n",
    "    grid_params = {\n",
    "               'n_estimators': ([10, 500], 'int'),  # Number of trees, 100-500 is usually sufficient \n",
    "               'max_depth': ([2, 10], 'int'),  # Depth of the tree, 5-6 is usually default\n",
    "               'max_features': ([\"sqrt\", \"log2\"], 'cat'),  # Number of variables randomly sampled as candidates at each split, sqrt(p) is usually the default\n",
    "               'learning_rate': ([0.05, 0.5], 'float'), # Learing rate. Shrinkage is used for reducing, or shrinking, the impact of each additional fitted \n",
    "                # base-learner (tree). It reduces the size of incremental steps and thus penalizes the importance of each consecutive iteration. \n",
    "                # The intuition behind this technique is that it is better to improve a model by taking many small steps than by taking fewer large steps. \n",
    "                # If one of the boosting iterations turns out to be erroneous, its negative impact can be easily corrected in subsequent steps. \n",
    "                # High learn rates and especially values close to 1.0 typically result in overfit models with poor performance. \n",
    "                # Values much smaller than .01 significantly slow down the learning process and might be reserved for overnight runs. 0.10 default\n",
    "               'min_samples_leaf': ([0.01, 0.4], 'float'), # Minimum number of observations in each leaf, 10 is usually default\n",
    "               'min_samples_split': ([0.01, 0.4], 'float'), # The minimum number of observations required to split an internal node\n",
    "            # If int then min_samples_split is the minimum number of observations\n",
    "            # If float then then minimum number of observations is a fraction and ceil(min_samples_split * n_samples) \n",
    "               'subsample': ([0.5, 1.0], 'float'), # The fraction of samples from n_estimators\n",
    "               'criterion': ([\"friedman_mse\",  \"squared_error\"], 'cat')}, # The function to measure the quality of a split\n",
    "    #        , \"loss\":[\"deviance\"]      \n",
    "    dev_df = data['data_{}'.format(sample_values_solution[0])], # dev_df: Development sample that this will analysis will be performed\n",
    "    feats = feats_best_gbm_back, # List of predictor names\n",
    "    target = target_variable_name, # Target variable name\n",
    "    weight_variable = weight_variable_name, # Weight variable name\n",
    "    random_state = 42, # If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.\n",
    "    n_splits_kfold = 2, # Number of cross-validation splits\n",
    "    n_repeats_kfold = 1, # Number that KFold cross-validation will be repeated\n",
    "    n_random_grids = 100, # Number of grid searches from Optuna. The more searches, the longer that the algorithm will take to complete. \n",
    "    timeout = 100000, # Time in seconds that Optuna will stop\n",
    "    n_jobs = -1 # Number of jobs to run in parallel\n",
    "    )\n",
    "\n",
    "gbm_optuna_search_model.optimize()\n",
    "\n",
    "# Save the best parameters\n",
    "opt_params_optuna = gbm_optuna_search_model.best_params()\n",
    "opt_loss_optuna = gbm_optuna_search_model.best_score()\n",
    "print('Best Hyperparameters (Optuna): ', opt_params_optuna)\n",
    "print('Mean loss function for cross-validation (Optuna): ', opt_loss_optuna)\n",
    "\n",
    "gbm_optuna_search_model.train_best_model()\n",
    "\n",
    "# Compute the score for the OOT data\n",
    "for k in data.keys():\n",
    "    data[k].loc[:, 'gbm_score_optuna_numeric'] = gbm_optuna_search_model.predict_probabilities(X_test = data[k][feats_best_gbm_back])[:, 1]\n",
    "    data[k].loc[:, 'gbm_score_optuna_binary'] = list(map(round, data[k].loc[:, 'gbm_score_optuna_numeric']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682b932b-2f7f-4697-8f8a-423516dae393",
   "metadata": {},
   "source": [
    "## Calculate GBM feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e068b8-3c48-40f8-9d3d-80a18cedc3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GBM algorithm\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Define the GBM model\n",
    "opt_params = opt_params_optuna\n",
    "gbm = GradientBoostingClassifier(n_estimators=[x[1] for x in list(opt_params.items()) if x[0]=='n_estimators'][0], \n",
    "                                 max_depth=[x[1] for x in list(opt_params.items()) if x[0]=='max_depth'][0], \n",
    "                                 max_features=[x[1] for x in list(opt_params.items()) if x[0]=='max_features'][0], \n",
    "                                 learning_rate=[x[1] for x in list(opt_params.items()) if x[0]=='learning_rate'][0], \n",
    "                                 min_samples_leaf=[x[1] for x in list(opt_params.items()) if x[0]=='min_samples_leaf'][0], \n",
    "                                 min_samples_split=[x[1] for x in list(opt_params.items()) if x[0]=='min_samples_split'][0], \n",
    "                                 subsample=[x[1] for x in list(opt_params.items()) if x[0]=='subsample'][0], \n",
    "                                 criterion=[x[1] for x in list(opt_params.items()) if x[0]=='criterion'][0],                                 \n",
    "                                 random_state=0)\n",
    "\n",
    "# Train the model\n",
    "gbm_model = mb.fit_model_weight(data['data_{}'.format(sample_values_solution[0])], \n",
    "                                feats_best_gbm_back, \n",
    "                                target_variable_name, \n",
    "                                weight_variable_name, \n",
    "                                gbm, \n",
    "                                data_path + '/output/' + 'gbm.pkl')\n",
    "\n",
    "# Calculate feature importance\n",
    "feat_imprtnce_dictnry_gbm = mb.feature_imp(gbm_model, feats_best_gbm_back, data_path, 'Feature_importance_GBM.csv')\n",
    "display(feat_imprtnce_dictnry_gbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f75b59-2882-4dbc-8e73-3d16cb2a84b8",
   "metadata": {},
   "source": [
    "## Produce GBM reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6b5fa7-4adc-4479-a9e1-f5865943c708",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_iterative_report_class = rp.binary_regression_report(\n",
    "    predictions_dictionary = data, \n",
    "    target_variable = target_variable_name, \n",
    "    predicted_score_numeric = 'gbm_score_iterative_numeric', \n",
    "    amount_variable_name = amount_variable_name_solution, \n",
    "    weight_variable_name = weight_variable_name_solution, \n",
    "    sample_values_dict = sample_values_dict, \n",
    "    select_top_percent = 100, \n",
    "    n_bands = 10, \n",
    "    rows = 10, \n",
    "    data_path = data_path\n",
    "    )\n",
    "gbm_iterative_eval = gbm_iterative_report_class.get_evaluation(predicted_score_binary = 'gbm_score_iterative_binary', \n",
    "                                                       filename = 'evaluation_metrics_GBM_iterative.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51872b7c-556c-4440-9df9-bdc6bc994dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_random_report_class = rp.binary_regression_report(\n",
    "    predictions_dictionary = data, \n",
    "    target_variable = target_variable_name, \n",
    "    predicted_score_numeric = 'gbm_score_random_numeric', \n",
    "    amount_variable_name = amount_variable_name_solution, \n",
    "    weight_variable_name = weight_variable_name_solution, \n",
    "    sample_values_dict = sample_values_dict, \n",
    "    select_top_percent = 100, \n",
    "    n_bands = 10, \n",
    "    rows = 10, \n",
    "    data_path = data_path\n",
    "    )\n",
    "gbm_random_eval = gbm_random_report_class.get_evaluation(predicted_score_binary = 'gbm_score_random_binary', \n",
    "                                                       filename = 'evaluation_metrics_GBM_random.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbb743c-a05c-42ae-b05c-647c24d9d5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_fixed_report_class = rp.binary_regression_report(\n",
    "    predictions_dictionary = data, \n",
    "    target_variable = target_variable_name, \n",
    "    predicted_score_numeric = 'gbm_score_fixed_numeric', \n",
    "    amount_variable_name = amount_variable_name_solution, \n",
    "    weight_variable_name = weight_variable_name_solution, \n",
    "    sample_values_dict = sample_values_dict, \n",
    "    select_top_percent = 100, \n",
    "    n_bands = 10, \n",
    "    rows = 10, \n",
    "    data_path = data_path\n",
    "    )\n",
    "gbm_fixed_eval = gbm_fixed_report_class.get_evaluation(predicted_score_binary = 'gbm_score_fixed_binary', \n",
    "                                                       filename = 'evaluation_metrics_GBM_fixed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b246369-64a2-4fb2-bafa-67ee75e363bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_optuna_report_class = rp.binary_regression_report(\n",
    "    predictions_dictionary = data, \n",
    "    target_variable = target_variable_name, \n",
    "    predicted_score_numeric = 'gbm_score_optuna_numeric', \n",
    "    amount_variable_name = amount_variable_name_solution, \n",
    "    weight_variable_name = weight_variable_name_solution, \n",
    "    sample_values_dict = sample_values_dict, \n",
    "    select_top_percent = 100, \n",
    "    n_bands = 10, \n",
    "    rows = 10, \n",
    "    data_path = data_path\n",
    "    )\n",
    "gbm_optuna_eval = gbm_optuna_report_class.get_evaluation(predicted_score_binary = 'gbm_score_optuna_binary', \n",
    "                                                       filename = 'evaluation_metrics_GBM_optuna.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059e5dd1-0f13-407b-b30e-4397e82e96ea",
   "metadata": {},
   "source": [
    "## Calculate GBM Lifting table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9988a586-4501-40f7-9a6d-2b0ea68862f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_report_class = gbm_optuna_report_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff20579-31d4-49e2-b608-c205baab47ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Lift table\n",
    "binary_report_lift_table = binary_report_class.create_lift_table(filename = 'lift_table_GBM_')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f167f99-0875-47ec-866d-feed72d60316",
   "metadata": {},
   "source": [
    "## Plots GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4220bf2-3f44-48a5-a4ff-73f68f109fd2",
   "metadata": {},
   "source": [
    "### Plot GBM Detection rate vs. Population Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c77fa9-b292-4c6b-ac62-6e1a8015837d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder, if it doesn't exist\n",
    "folder_name = 'graphs_GBM'\n",
    "ufun.create_folder(data_path = data_path, \n",
    "                   folder_name = 'output/{}'.format(folder_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94bffb6-782a-4384-93d5-2d8461e86754",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_report_class.plot_ADR_Quantile(\n",
    "        folder_name = folder_name,\n",
    "        xlim=None, \n",
    "        ylim=None\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b548c1-5172-4714-bf30-8a5f82154260",
   "metadata": {},
   "source": [
    "### Plot GBM Cum. Detection rate vs. Population Distribution (Gains chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02398e4e-8df6-40b5-a292-2c3029501f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_report_class.plot_cADR_Quantile(\n",
    "        folder_name = folder_name,\n",
    "        xlim=None, \n",
    "        ylim=None\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a61f8e9-89d6-4c7c-b04d-fb489b5e892b",
   "metadata": {},
   "source": [
    "### Plot GBM FPR vs. Population Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32899af3-8927-4bbd-a4a7-99900f8cec47",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_report_class.plot_FPR_Quantile(\n",
    "        folder_name = folder_name,\n",
    "        xlim=None, \n",
    "        ylim=None\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c45e25b-aa61-4367-881b-85c47c40b8dd",
   "metadata": {},
   "source": [
    "### Plot GBM Cum. FPR vs. Population Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296449fe-da47-4c94-80e8-06d58477c5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_report_class.plot_cFPR_Quantile(\n",
    "        folder_name = folder_name,\n",
    "        xlim=None, \n",
    "        ylim=None\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250f2793-4fcb-48ea-bf31-6ea606699fac",
   "metadata": {},
   "source": [
    "### Plot GBM ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0949ca7a-f8ab-4276-9e77-7537f1fcb327",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_report_class.plot_ROC_curve(folder_name = folder_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d0b963-2f4a-4c81-8348-a8f1c0ec9f5f",
   "metadata": {},
   "source": [
    "### Plot GBM Precision Recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34447e71-efd7-4a30-b00e-86020bba6572",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "binary_report_class.plot_precision_recall_curve(folder_name = folder_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050dd993-fb29-4966-a884-b46a8d4a462e",
   "metadata": {},
   "source": [
    "### Plot GBM F1 score, Accuracy, Sensitivity, Specificity, Precision vs Cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db3f4b4-1a5f-4c36-976b-9f0aab16e8f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "binary_report_class.plot_cutoffs(\n",
    "        folder_name = folder_name,\n",
    "        n_bands = 100, # Number of bands between 0 and 1\n",
    "        return_table=False # Set to True in order to return the table that produced the graph, otherwise set to False\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b33513d-9af7-4389-b337-1593903c2977",
   "metadata": {},
   "source": [
    "# Light Gradient Boosting Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9720b5af-fc7d-4f3d-a685-32c672e53727",
   "metadata": {},
   "source": [
    "## Feature selection lightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15eba88d-1221-4ca2-bd34-89769b0d8753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide input data to feature selection class\n",
    "select = fe.SelectBest_weight(df=data['data_{}'.format(sample_values_solution[0])], \n",
    "                              target=target_variable_name, \n",
    "                              weight=weight_variable_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56eea40-27c0-47bb-93a7-27abad309817",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select top features to apply a first screening\n",
    "from lightgbm import LGBMClassifier\n",
    "model_lgbm = LGBMClassifier(boosting_type='gbdt',  objective='binary', n_estimators=200, \n",
    "                            learning_rate=0.1, max_depth=5, random_state=1234, n_jobs=6)\n",
    "lgbm_importance, feats_best_lgbm = select.top_lgbm_feat(feats=keep_num_vars_gini, model=model_lgbm, n=15)\n",
    "print('The top lightGBM features are: \\n', feats_best_lgbm)\n",
    "lgbm_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b67a85-8c53-42a6-8238-479577d35055",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "start = timeit.default_timer()\n",
    "\n",
    "# Backward selection with lightGBM model: In each step a feature is selected to remove - this removal maximizes Gini\n",
    "feats_best_lgbm_back = select.backward_recur(feats=feats_best_lgbm, \n",
    "                                             oos=data['data_{}'.format(sample_values_solution[1])], \n",
    "                                             model=model_lgbm, \n",
    "                                             min_feats=10, \n",
    "                                             classification=True)\n",
    "print(feats_best_lgbm_back)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Execution time in seconds: ', stop - start)  \n",
    "# Execution time in seconds:  11.858331700000008"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954b4220-da60-4d70-8a6c-29de02e80e4e",
   "metadata": {},
   "source": [
    "## Grid search and train lightGBM machine learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31d138f-5135-413f-95da-2d5ea69fa1c2",
   "metadata": {},
   "source": [
    "### Grid search based on iterative step search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdf8e49-7339-4d1b-a1af-c575c0c19553",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameter tuning: The process works as follows:\n",
    "# 1) Iterate n_estimators (grid) for the fixed values of max_depth, learning_rate, etc (params). Select the value of n_estimators that minimizes the loss function. \n",
    "# 2) Keep the n_estimators value from the previous step. Iterate max_depth (grid) for the fixed value of learning_rate, etc (params). Select the value of max_depth that minimizes the loss function. \n",
    "# 3) Keep the max_depth value from the previous step. Iterate learning_rate (grid) for the fixed value of the remaining parameters (params). Select the value of max_features that minimizes the loss function. \n",
    "# 4) ...\n",
    "import math\n",
    "# 38 models in total\n",
    "grid = {'n_estimators':[10, 20, 50, 100, 200], # Number of trees, 100-500 is usually sufficient\n",
    "       'max_depth':[2, 5, 10], # Depth of the tree, 5-6 is usually default\n",
    "       'learning_rate':[0.05, 0.1, 0.2, 0.5], # Learing rate. Shrinkage is used for reducing, or shrinking, the impact of each additional fitted \n",
    "# base-learner (tree). It reduces the size of incremental steps and thus penalizes the importance of each consecutive iteration. \n",
    "# The intuition behind this technique is that it is better to improve a model by taking many small steps than by taking fewer large steps. \n",
    "# If one of the boosting iterations turns out to be erroneous, its negative impact can be easily corrected in subsequent steps. \n",
    "# High learn rates and especially values close to 1.0 typically result in overfit models with poor performance. \n",
    "# Values much smaller than .01 significantly slow down the learning process and might be reserved for overnight runs. 0.10 default\n",
    "        'min_child_samples': [10, 100, 500, 1000, 2000, 5000], # Minimum number of observations in each leaf, 10 is usually default\n",
    "        'subsample': [0.5, 0.7, 0.8, 0.9, 0.95, 1.0], # The fraction of samples from n_estimators\n",
    "        'colsample_bytree': np.linspace(0.01, 0.4, 10), # Subsample ratio of columns when constructing each tree\n",
    "        'eval_metric': ['l2', 'l1', 'logloss', 'ndcg'], # If string, it should be a built-in evaluation metric to use\n",
    "        'boosting_type': ['gbdt', 'dart', 'goss'], # ‘gbdt’, traditional Gradient Boosting Decision Tree. ‘dart’, Dropouts meet Multiple Additive Regression Trees. ‘goss’, Gradient-based One-Side Sampling. ‘rf’, Random Forest\n",
    "        'num_leaves': [10, 20, 30, 50], # Maximum number of tree leaves \n",
    "        'bagging_fraction': [0.5, 0.7, 0.8, 0.9, 0.95, 1.0], # The fraction of observations to be used for each iteration\n",
    "        'reg_alpha': [0.1, 0.5], # L1 regularization term on weights\n",
    "        'reg_lambda': [0.1, 0.5], # L2 regularization term on weights\n",
    "        'lambda_l1': [0, 1, 1.5], # L1 regularization\n",
    "        'lambda_l2': [0, 1], # L2 regularization\n",
    "        'importance_type': ['split'], # The type of feature importance to be filled into feature_importances_. If ‘split’, result contains numbers of times the feature is used in a model. If ‘gain’, result contains total gains of splits which use the feature.\n",
    "        'objective': ['binary'] # Specify the learning task and the corresponding learning objective\n",
    "       }\n",
    "\n",
    "params = {'n_estimators': 100, 'max_depth':4, 'learning_rate':0.1, 'min_child_samples': 10, 'subsample': 1.0, \n",
    "          'colsample_bytree': math.ceil(math.sqrt(len(feats_best_lgbm_back)))/len(feats_best_lgbm_back),  'metric': 'l2', 'boosting_type': 'gbdt',\n",
    "          'num_leaves': 31, 'bagging_fraction':1, 'reg_alpha': 0, 'reg_lambda': 0, 'lambda_l1': 0, 'lambda_l2': 0,\n",
    "          'importance_type': 'split', 'objective': 'binary'\n",
    "         }            \n",
    "\n",
    "opt_params_iterative, loss = mb.step_search_weight(\n",
    "    estimator=LGBMClassifier, \n",
    "    params=params, \n",
    "    grid=grid, \n",
    "    target=target_variable_name,\n",
    "    weight=weight_variable_name, \n",
    "    dev=data['data_{}'.format(sample_values_solution[0])], \n",
    "    val=data['data_{}'.format(sample_values_solution[1])], \n",
    "    keep=feats_best_lgbm_back)\n",
    "\n",
    "print('\\n Best Parameters')\n",
    "print(opt_params_iterative)\n",
    "print(loss)\n",
    "\n",
    "# Define the lightGBM model\n",
    "lightgbm_classifier = LGBMClassifier(n_estimators=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='n_estimators'][0], \n",
    "                                 max_depth=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='max_depth'][0], \n",
    "                                 learning_rate=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='learning_rate'][0], \n",
    "                                 min_child_samples=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='min_child_samples'][0], \n",
    "                                 subsample=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='subsample'][0], \n",
    "                                 colsample_bytree=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='colsample_bytree'][0], \n",
    "                                 metric=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='metric'][0], \n",
    "                                 boosting_type=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='boosting_type'][0], \n",
    "                                 num_leaves=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='num_leaves'][0], \n",
    "                                 bagging_fraction=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='bagging_fraction'][0], \n",
    "                                 reg_alpha=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='reg_alpha'][0], \n",
    "                                 reg_lambda=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='reg_lambda'][0], \n",
    "                                 lambda_l1=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='lambda_l1'][0], \n",
    "                                 lambda_l2=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='lambda_l2'][0], \n",
    "                                 importance_type=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='importance_type'][0], \n",
    "                                 objective=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='objective'][0], \n",
    "                                 random_state=1234, \n",
    "                                 n_jobs=6)\n",
    "\n",
    "# Train the model\n",
    "lightgbm_model = mb.fit_model_weight(data['data_{}'.format(sample_values_solution[0])], \n",
    "                                     feats_best_lgbm_back, \n",
    "                                     target_variable_name, \n",
    "                                     weight_variable_name, \n",
    "                                     lightgbm_classifier, \n",
    "                                     data_path + '/output/' + 'lgbm_iterative.pkl')\n",
    "\n",
    "# Compute the score for the OOT data\n",
    "for k in data.keys():\n",
    "    data[k].loc[:, 'lightgbm_score_iterative_numeric'] = lightgbm_model.predict_proba(X = data[k][feats_best_lgbm_back])[:, 1]\n",
    "    data[k].loc[:, 'lightgbm_score_iterative_binary'] = list(map(round, data[k].loc[:, 'lightgbm_score_iterative_numeric']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c3f6d8-cc5b-4910-880f-558366264d10",
   "metadata": {},
   "source": [
    "### Grid search based on RandomizedSearchCV - also uses cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4255acca-8ff3-40b8-93ef-5ec736d3a066",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1,536 models in total\n",
    "grid = {'n_estimators':[50, 100, 200], # Number of trees, 100-500 is usually sufficient\n",
    "       'max_depth':[2, 5], # Depth of the tree, 5-6 is usually default\n",
    "       'learning_rate':[0.1, 0.2], # Learing rate. Shrinkage is used for reducing, or shrinking, the impact of each additional fitted \n",
    "# base-learner (tree). It reduces the size of incremental steps and thus penalizes the importance of each consecutive iteration. \n",
    "# The intuition behind this technique is that it is better to improve a model by taking many small steps than by taking fewer large steps. \n",
    "# If one of the boosting iterations turns out to be erroneous, its negative impact can be easily corrected in subsequent steps. \n",
    "# High learn rates and especially values close to 1.0 typically result in overfit models with poor performance. \n",
    "# Values much smaller than .01 significantly slow down the learning process and might be reserved for overnight runs. 0.10 default\n",
    "        'min_child_samples': [10, 100], # Minimum number of observations in each leaf, 10 is usually default\n",
    "        'subsample': [0.95, 1.0], # The fraction of samples from n_estimators\n",
    "        'colsample_bytree': np.linspace(0.20, 0.23, 2), # Subsample ratio of columns when constructing each tree\n",
    "        'metric': ['l2'], # If string, it should be a built-in evaluation metric to use\n",
    "        'boosting_type': ['gbdt'], # ‘gbdt’, traditional Gradient Boosting Decision Tree. ‘dart’, Dropouts meet Multiple Additive Regression Trees. ‘goss’, Gradient-based One-Side Sampling. ‘rf’, Random Forest\n",
    "        'num_leaves': [30, 35], # Maximum number of tree leaves \n",
    "        'bagging_fraction': [0.95, 1.0], # The fraction of observations to be used for each iteration\n",
    "        'reg_alpha': [0, 0.1], # L1 regularization term on weights\n",
    "        'reg_lambda': [0, 0.1], # L2 regularization term on weights\n",
    "        'lambda_l1': [1], # L1 regularization\n",
    "        'lambda_l2': [0], # L2 regularization\n",
    "        'importance_type': ['split'], # The type of feature importance to be filled into feature_importances_. If ‘split’, result contains numbers of times the feature is used in a model. If ‘gain’, result contains total gains of splits which use the feature.\n",
    "        'objective': ['binary'] # Specify the learning task and the corresponding learning objective\n",
    "       }\n",
    "\n",
    "lightgbm_grid_random_search = mb.grid_search_cv(\n",
    "    n_splits = 2, # Number of cross-validation splits\n",
    "    classifier = LGBMClassifier, # Classifier name, e.g. RandomForestClassifier\n",
    "    keras_function = None, # Define Keras function. If Keras is not used, then leave this parameter blank\n",
    "    grid_params = grid, # Grid space\n",
    "    dev_df = data['data_{}'.format(sample_values_solution[0])],  # Development sample that this will analysis will be performed\n",
    "    feats = feats_best_lgbm_back, # List of predictor names\n",
    "    target = target_variable_name, # Target variable name\n",
    "    weight_variable = weight_variable_name, # Weight variable name\n",
    "    randomized_search = True, # Set to True if randomized grid search will be performed, or to False if exhaustive grid search will be performed\n",
    "    n_random_grids = 100, # Number of grid searches when randomized_search=True. If randomized_search=False, then this parameter is not applicable\n",
    "    random_state = None, # If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.\n",
    "    n_jobs = 8 # Number of jobs to run in parallel\n",
    ")\n",
    "\n",
    "# Save the best parameters\n",
    "opt_params_random = lightgbm_grid_random_search.best_params_\n",
    "print('The best hyperparameter set is:', opt_params_random)\n",
    "print('Mean loss function for cross-validation test data: ', -lightgbm_grid_random_search.cv_results_['mean_test_score'][lightgbm_grid_random_search.best_index_])\n",
    "print('Standard deviation loss function for cross-validation test data: ', lightgbm_grid_random_search.cv_results_['std_test_score'][lightgbm_grid_random_search.best_index_])\n",
    "#print('Mean Gini for OOT data', 2*lightgbm_grid_random_search.score(oos[feats_best_lgbm_back], oos[target_variable_name])-1)\n",
    "\n",
    "rp.plot_cross_validation_score(model=lightgbm_grid_random_search)\n",
    "\n",
    "# Compute the score for the OOT data\n",
    "for k in data.keys():\n",
    "    data[k].loc[:, 'lightgbm_score_random_numeric'] = lightgbm_grid_random_search.predict_proba(X = data[k][feats_best_lgbm_back])[:, 1]\n",
    "    data[k].loc[:, 'lightgbm_score_random_binary'] = list(map(round, data[k].loc[:, 'lightgbm_score_random_numeric']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e643f29-4b48-41c1-b38b-14ced4a76e4f",
   "metadata": {},
   "source": [
    "### Grid search based on GridSearchCV - also uses cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c971343f-8393-49d5-895a-ec7f55d1c2c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1,536 models in total\n",
    "grid = {'n_estimators':[50, 100, 200], # Number of trees, 100-500 is usually sufficient\n",
    "       'max_depth':[2, 5], # Depth of the tree, 5-6 is usually default\n",
    "       'learning_rate':[0.1, 0.2], # Learing rate. Shrinkage is used for reducing, or shrinking, the impact of each additional fitted \n",
    "# base-learner (tree). It reduces the size of incremental steps and thus penalizes the importance of each consecutive iteration. \n",
    "# The intuition behind this technique is that it is better to improve a model by taking many small steps than by taking fewer large steps. \n",
    "# If one of the boosting iterations turns out to be erroneous, its negative impact can be easily corrected in subsequent steps. \n",
    "# High learn rates and especially values close to 1.0 typically result in overfit models with poor performance. \n",
    "# Values much smaller than .01 significantly slow down the learning process and might be reserved for overnight runs. 0.10 default\n",
    "        'min_child_samples': [10, 100], # Minimum number of observations in each leaf, 10 is usually default\n",
    "        'subsample': [0.95, 1.0], # The fraction of samples from n_estimators\n",
    "        'colsample_bytree': np.linspace(0.20, 0.23, 2), # Subsample ratio of columns when constructing each tree\n",
    "        'metric': ['l2'], # If string, it should be a built-in evaluation metric to use\n",
    "        'boosting_type': ['gbdt'], # ‘gbdt’, traditional Gradient Boosting Decision Tree. ‘dart’, Dropouts meet Multiple Additive Regression Trees. ‘goss’, Gradient-based One-Side Sampling. ‘rf’, Random Forest\n",
    "        'num_leaves': [30, 35], # Maximum number of tree leaves \n",
    "        'bagging_fraction': [0.95, 1.0], # The fraction of observations to be used for each iteration\n",
    "        'reg_alpha': [0, 0.1], # L1 regularization term on weights\n",
    "        'reg_lambda': [0, 0.1], # L2 regularization term on weights\n",
    "        'lambda_l1': [1], # L1 regularization\n",
    "        'lambda_l2': [0], # L2 regularization\n",
    "        'importance_type': ['split'], # The type of feature importance to be filled into feature_importances_. If ‘split’, result contains numbers of times the feature is used in a model. If ‘gain’, result contains total gains of splits which use the feature.\n",
    "        'objective': ['binary'] # Specify the learning task and the corresponding learning objective\n",
    "       }\n",
    "\n",
    "lightgbm_grid_fixed_search = mb.grid_search_cv(\n",
    "    n_splits = 2, # Number of cross-validation splits\n",
    "    classifier = LGBMClassifier, # Classifier name, e.g. RandomForestClassifier\n",
    "    keras_function = None, # Define Keras function. If Keras is not used, then leave this parameter blank\n",
    "    grid_params = grid, # Grid space\n",
    "    dev_df = data['data_{}'.format(sample_values_solution[0])],  # Development sample that this will analysis will be performed\n",
    "    feats = feats_best_lgbm_back, # List of predictor names\n",
    "    target = target_variable_name, # Target variable name\n",
    "    weight_variable = weight_variable_name, # Weight variable name\n",
    "    randomized_search = False, # Set to True if randomized grid search will be performed, or to False if exhaustive grid search will be performed\n",
    "    n_random_grids = 1, # Number of grid searches when randomized_search=True. If randomized_search=False, then this parameter is not applicable\n",
    "    random_state = None, # If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.\n",
    "    n_jobs = 8 # Number of jobs to run in parallel\n",
    ")\n",
    "\n",
    "# Save the best parameters\n",
    "opt_params_fixed = lightgbm_grid_fixed_search.best_params_\n",
    "print('The best hyperparameter set is:', opt_params_fixed)\n",
    "print('Mean loss function for cross-validation test data: ', -lightgbm_grid_fixed_search.cv_results_['mean_test_score'][lightgbm_grid_fixed_search.best_index_])\n",
    "print('Standard deviation loss function for cross-validation test data: ', lightgbm_grid_fixed_search.cv_results_['std_test_score'][lightgbm_grid_fixed_search.best_index_])\n",
    "#print('Mean Gini for OOT data', 2*lightgbm_grid_fixed_search.score(oos[feats_best_lgbm_back], oos[target_variable_name], sample_weight=oos[weight_variable_name])-1)\n",
    "\n",
    "rp.plot_cross_validation_score(model=lightgbm_grid_fixed_search)\n",
    "\n",
    "# Compute the score for the OOT data\n",
    "for k in data.keys():\n",
    "    data[k].loc[:, 'lightgbm_score_fixed_numeric'] = lightgbm_grid_fixed_search.predict_proba(X = data[k][feats_best_lgbm_back])[:, 1]\n",
    "    data[k].loc[:, 'lightgbm_score_fixed_binary'] = list(map(round, data[k].loc[:, 'lightgbm_score_fixed_numeric']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d373fc-ecb5-422c-811d-648e5dce87bb",
   "metadata": {},
   "source": [
    "### Grid search based on Optuna - also uses cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c040a5cc-31e7-4729-b9c2-76d89af1be59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lightgbm_optuna_search_model = mb.grid_search_optuna(\n",
    "    classifier = LGBMClassifier, # Classifier name, e.g. RandomForestClassifier\n",
    "    grid_params = {\n",
    "       'n_estimators': ([10, 200], 'int'), # Number of trees, 100-500 is usually sufficient\n",
    "       'max_depth': ([2, 10], 'int'), # Depth of the tree, 5-6 is usually default\n",
    "       'learning_rate': ([0.05, 0.5], 'float'), # Learing rate. Shrinkage is used for reducing, or shrinking, the impact of each additional fitted \n",
    "# base-learner (tree). It reduces the size of incremental steps and thus penalizes the importance of each consecutive iteration. \n",
    "# The intuition behind this technique is that it is better to improve a model by taking many small steps than by taking fewer large steps. \n",
    "# If one of the boosting iterations turns out to be erroneous, its negative impact can be easily corrected in subsequent steps. \n",
    "# High learn rates and especially values close to 1.0 typically result in overfit models with poor performance. \n",
    "# Values much smaller than .01 significantly slow down the learning process and might be reserved for overnight runs. 0.10 default\n",
    "        'min_child_samples': ([10, 5000], 'int'), # Minimum number of observations in each leaf, 10 is usually default\n",
    "        'subsample': ([0.5, 1.0], 'float'), # The fraction of samples from n_estimators\n",
    "        'colsample_bytree': ([0.01, 0.4], 'float'), # Subsample ratio of columns when constructing each tree\n",
    "        'metric': (['l2', 'l1', 'logloss'], 'cat'), # If string, it should be a built-in evaluation metric to use\n",
    "        'boosting_type': (['gbdt', 'dart', 'goss'], 'cat'), # ‘gbdt’, traditional Gradient Boosting Decision Tree. ‘dart’, Dropouts meet Multiple Additive Regression Trees. ‘goss’, Gradient-based One-Side Sampling. ‘rf’, Random Forest\n",
    "        'num_leaves': ([10, 50], 'int'), # Maximum number of tree leaves \n",
    "        'bagging_fraction': ([0.5, 1.0], 'float'), # The fraction of observations to be used for each iteration\n",
    "        'reg_alpha': ([0.1, 0.5], 'float'), # L1 regularization term on weights\n",
    "        'reg_lambda': ([0.1, 0.5], 'float'), # L2 regularization term on weights\n",
    "        'lambda_l1': ([0, 1.5], 'float'), # L1 regularization\n",
    "        'lambda_l2': ([0, 1], 'float'), # L2 regularization\n",
    "        'importance_type': (['split'], 'cat'), # The type of feature importance to be filled into feature_importances_. If ‘split’, result contains numbers of times the feature is used in a model. If ‘gain’, result contains total gains of splits which use the feature.\n",
    "        'objective': (['binary'], 'cat') # Specify the learning task and the corresponding learning objective\n",
    "    },\n",
    "    dev_df = data['data_{}'.format(sample_values_solution[0])], # dev_df: Development sample that this will analysis will be performed\n",
    "    feats = feats_best_lgbm_back, # List of predictor names\n",
    "    target = target_variable_name, # Target variable name\n",
    "    weight_variable = weight_variable_name, # Weight variable name\n",
    "    random_state = 42, # If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.\n",
    "    n_splits_kfold = 2, # Number of cross-validation splits\n",
    "    n_repeats_kfold = 1, # Number that KFold cross-validation will be repeated\n",
    "    n_random_grids = 100, # Number of grid searches from Optuna. The more searches, the longer that the algorithm will take to complete. \n",
    "    timeout = 100000, # Time in seconds that Optuna will stop\n",
    "    n_jobs = -1 # Number of jobs to run in parallel\n",
    "    )\n",
    "\n",
    "lightgbm_optuna_search_model.optimize()\n",
    "\n",
    "# Save the best parameters\n",
    "opt_params_optuna = lightgbm_optuna_search_model.best_params()\n",
    "opt_loss_optuna = lightgbm_optuna_search_model.best_score()\n",
    "print('Best Hyperparameters (Optuna): ', opt_params_optuna)\n",
    "print('Mean loss function for cross-validation (Optuna): ', opt_loss_optuna)\n",
    "\n",
    "lightgbm_optuna_search_model.train_best_model()\n",
    "\n",
    "# Compute the score for the OOT data\n",
    "for k in data.keys():\n",
    "    data[k].loc[:, 'lightgbm_score_optuna_numeric'] = lightgbm_optuna_search_model.predict_probabilities(X_test = data[k][feats_best_lgbm_back])[:, 1]\n",
    "    data[k].loc[:, 'lightgbm_score_optuna_binary'] = list(map(round, data[k].loc[:, 'lightgbm_score_optuna_numeric']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee6bae8-3f40-46e6-ae15-15e4cf08bf22",
   "metadata": {},
   "source": [
    "## Calculate lightGBM feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f3df83-1edf-4d66-aeab-4b5c9844aaf4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the lightGBM model\n",
    "opt_params = opt_params_optuna\n",
    "lightgbm_classifier = LGBMClassifier(n_estimators=[x[1] for x in list(opt_params.items()) if x[0]=='n_estimators'][0], \n",
    "                                 max_depth=[x[1] for x in list(opt_params.items()) if x[0]=='max_depth'][0], \n",
    "                                 learning_rate=[x[1] for x in list(opt_params.items()) if x[0]=='learning_rate'][0], \n",
    "                                 min_child_samples=[x[1] for x in list(opt_params.items()) if x[0]=='min_child_samples'][0], \n",
    "                                 subsample=[x[1] for x in list(opt_params.items()) if x[0]=='subsample'][0], \n",
    "                                 colsample_bytree=[x[1] for x in list(opt_params.items()) if x[0]=='colsample_bytree'][0], \n",
    "                                 metric=[x[1] for x in list(opt_params.items()) if x[0]=='metric'][0], \n",
    "                                 boosting_type=[x[1] for x in list(opt_params.items()) if x[0]=='boosting_type'][0], \n",
    "                                 num_leaves=[x[1] for x in list(opt_params.items()) if x[0]=='num_leaves'][0], \n",
    "                                 bagging_fraction=[x[1] for x in list(opt_params.items()) if x[0]=='bagging_fraction'][0], \n",
    "                                 reg_alpha=[x[1] for x in list(opt_params.items()) if x[0]=='reg_alpha'][0], \n",
    "                                 reg_lambda=[x[1] for x in list(opt_params.items()) if x[0]=='reg_lambda'][0], \n",
    "                                 lambda_l1=[x[1] for x in list(opt_params.items()) if x[0]=='lambda_l1'][0], \n",
    "                                 lambda_l2=[x[1] for x in list(opt_params.items()) if x[0]=='lambda_l2'][0], \n",
    "                                 importance_type=[x[1] for x in list(opt_params.items()) if x[0]=='importance_type'][0], \n",
    "                                 objective=[x[1] for x in list(opt_params.items()) if x[0]=='objective'][0], \n",
    "                                 random_state=1234, \n",
    "                                 n_jobs=6)\n",
    "\n",
    "# Train the model\n",
    "lightgbm_model = mb.fit_model_weight(data['data_{}'.format(sample_values_solution[0])], \n",
    "                                     feats_best_lgbm_back, \n",
    "                                     target_variable_name, \n",
    "                                     weight_variable_name, \n",
    "                                     lightgbm_classifier, \n",
    "                                     data_path + '/output/' + 'lgbm.pkl')\n",
    "\n",
    "# Calculate feature importance\n",
    "feat_imprtnce_dictnry_lgbm = mb.feature_imp(lightgbm_model, feats_best_lgbm_back, data_path, 'Feature_importance_lightGBM.csv')\n",
    "display(feat_imprtnce_dictnry_lgbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ba5c0d-c70e-4063-919d-aa3e6aa2a1c4",
   "metadata": {},
   "source": [
    "## Produce lightGBM reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0d90c3-4515-41ec-9580-02c0d358bbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lightgbm_iterative_report_class = rp.binary_regression_report(\n",
    "    predictions_dictionary = data, \n",
    "    target_variable = target_variable_name, \n",
    "    predicted_score_numeric = 'lightgbm_score_iterative_numeric', \n",
    "    amount_variable_name = amount_variable_name_solution, \n",
    "    weight_variable_name = weight_variable_name_solution, \n",
    "    sample_values_dict = sample_values_dict, \n",
    "    select_top_percent = 100, \n",
    "    n_bands = 10, \n",
    "    rows = 10, \n",
    "    data_path = data_path\n",
    "    )\n",
    "lightgbm_iterative_eval = lightgbm_iterative_report_class.get_evaluation(predicted_score_binary = 'lightgbm_score_iterative_binary', \n",
    "                                                       filename = 'evaluation_metrics_lightGBM_iterative.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b674b8-bc43-4a2c-8423-29607f5c60c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lightgbm_random_report_class = rp.binary_regression_report(\n",
    "    predictions_dictionary = data, \n",
    "    target_variable = target_variable_name, \n",
    "    predicted_score_numeric = 'lightgbm_score_random_numeric', \n",
    "    amount_variable_name = amount_variable_name_solution, \n",
    "    weight_variable_name = weight_variable_name_solution, \n",
    "    sample_values_dict = sample_values_dict, \n",
    "    select_top_percent = 100, \n",
    "    n_bands = 10, \n",
    "    rows = 10, \n",
    "    data_path = data_path\n",
    "    )\n",
    "lightgbm_random_eval = lightgbm_random_report_class.get_evaluation(predicted_score_binary = 'lightgbm_score_random_binary', \n",
    "                                                       filename = 'evaluation_metrics_lightGBM_random.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d183be-29e7-4159-b5a8-f6df1e9b5a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "lightgbm_fixed_report_class = rp.binary_regression_report(\n",
    "    predictions_dictionary = data, \n",
    "    target_variable = target_variable_name, \n",
    "    predicted_score_numeric = 'lightgbm_score_fixed_numeric', \n",
    "    amount_variable_name = amount_variable_name_solution, \n",
    "    weight_variable_name = weight_variable_name_solution, \n",
    "    sample_values_dict = sample_values_dict, \n",
    "    select_top_percent = 100, \n",
    "    n_bands = 10, \n",
    "    rows = 10, \n",
    "    data_path = data_path\n",
    "    )\n",
    "lightgbm_fixed_eval = lightgbm_fixed_report_class.get_evaluation(predicted_score_binary = 'lightgbm_score_fixed_binary', \n",
    "                                                       filename = 'evaluation_metrics_lightGBM_fixed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5a313e-828b-47cc-93e3-d4e5918425d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lightgbm_optuna_report_class = rp.binary_regression_report(\n",
    "    predictions_dictionary = data, \n",
    "    target_variable = target_variable_name, \n",
    "    predicted_score_numeric = 'lightgbm_score_optuna_numeric', \n",
    "    amount_variable_name = amount_variable_name_solution, \n",
    "    weight_variable_name = weight_variable_name_solution, \n",
    "    sample_values_dict = sample_values_dict, \n",
    "    select_top_percent = 100, \n",
    "    n_bands = 10, \n",
    "    rows = 10, \n",
    "    data_path = data_path\n",
    "    )\n",
    "lightgbm_optuna_eval = lightgbm_optuna_report_class.get_evaluation(predicted_score_binary = 'lightgbm_score_optuna_binary', \n",
    "                                                       filename = 'evaluation_metrics_lightGBM_optuna.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7992427-918b-4533-ac92-a4d9ac21a22f",
   "metadata": {},
   "source": [
    "## Calculate lightGBM Lifting table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe379f4-dcb0-4c74-b5bf-e390b69a349b",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_report_class = lightgbm_optuna_report_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb2b184-be67-407e-91a1-ee8d71fe5dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Lift table\n",
    "binary_report_lift_table = binary_report_class.create_lift_table(filename = 'lift_table_lightGBM_')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb5f611-6d73-453e-9f95-3b4041ea1e23",
   "metadata": {},
   "source": [
    "## Plots lightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47a62a8-fb5b-4ded-aac9-e98853d58ed6",
   "metadata": {},
   "source": [
    "### Plot lightGBM Detection rate vs. Population Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6feea226-403b-4a22-a13d-aab2d77dd065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder, if it doesn't exist\n",
    "folder_name = 'graphs_lightGBM'\n",
    "ufun.create_folder(data_path = data_path, \n",
    "                   folder_name = 'output/{}'.format(folder_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaae0579-928f-4e7a-b0d9-414e5105d38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_report_class.plot_ADR_Quantile(\n",
    "        folder_name = folder_name,\n",
    "        xlim=None, \n",
    "        ylim=None\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59468f9b-2e70-40da-81b6-eb5950fdef7a",
   "metadata": {},
   "source": [
    "### Plot lightGBM Cum. Detection rate vs. Population Distribution (Gains chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cbfad3-918c-4e7f-b94a-01481d0f7f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_report_class.plot_cADR_Quantile(\n",
    "        folder_name = folder_name,\n",
    "        xlim=None, \n",
    "        ylim=None\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1ceb5a-2a4e-47cb-a1d2-30b1c4fa22b1",
   "metadata": {},
   "source": [
    "### Plot lightGBM FPR vs. Population Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b47e69-bee9-4b7a-ba59-6945dc444324",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_report_class.plot_FPR_Quantile(\n",
    "        folder_name = folder_name,\n",
    "        xlim=None, \n",
    "        ylim=None\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2adddc-34cd-4be2-8862-9e90f8f3d526",
   "metadata": {},
   "source": [
    "### Plot lightGBM Cum. FPR vs. Population Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15791592-3cb7-46ca-b775-761ce353c3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_report_class.plot_cFPR_Quantile(\n",
    "        folder_name = folder_name,\n",
    "        xlim=None, \n",
    "        ylim=None\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ed11a3-e931-48c2-a632-062bf2032db7",
   "metadata": {},
   "source": [
    "### Plot lightGBM ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f85b10-5b05-4d0b-ad07-714107f9dae7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "binary_report_class.plot_ROC_curve(folder_name = folder_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd194b0-8624-4382-8382-e829df52bfb1",
   "metadata": {},
   "source": [
    "### Plot lightGBM Precision Recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3f3893-c11d-4e60-816a-960b59076a3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "binary_report_class.plot_precision_recall_curve(folder_name = folder_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09ce602-e557-4abc-b438-e5b4606b3b9f",
   "metadata": {},
   "source": [
    "### Plot lightGBM F1 score, Accuracy, Sensitivity, Specificity, Precision vs Cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de33f49-2f25-42cc-bb5b-bf936624d162",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_report_class.plot_cutoffs(\n",
    "        folder_name = folder_name,\n",
    "        n_bands = 100, # Number of bands between 0 and 1\n",
    "        return_table=False # Set to True in order to return the table that produced the graph, otherwise set to False\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc08a82b-b819-42c5-b0e4-18add2f7fa6e",
   "metadata": {},
   "source": [
    "# Keras NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82018125-283d-41d7-81c0-c02e0624c97f",
   "metadata": {},
   "source": [
    "## Feature selection Keras Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165455b2-8fe8-497c-b8a7-8958886d0f2f",
   "metadata": {},
   "source": [
    "### First find neural network hyperparameters for a network using RandomizedSearchCV, then select the top variables based on that network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874eb727-76d8-48d7-9db9-f2662c6ef0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "grid = [\n",
    "# SGD optimizer\n",
    "    {\n",
    "    'num_neurons': [math.floor((len(keep_num_vars_gini)+1)/2)]\n",
    "    , 'num_hidden_layers': [1, 2]\n",
    "    , 'input_dim': [data['data_{}'.format(sample_values_solution[0])][keep_num_vars_gini].shape[1]]\n",
    "    , 'kernel_initializer': ['normal', 'uniform']\n",
    "    , 'activation': ['relu', 'elu', 'linear', 'softplus']\n",
    "    , 'kernel_constraint': [0,1,2,3,4,5]\n",
    "    , 'dropout_rate': [0, 0.25, 0.5, 0.75]\n",
    "    , 'output_kernel_initializer': ['uniform']\n",
    "    , 'output_activation': ['sigmoid']\n",
    "    , 'loss': ['binary_crossentropy']\n",
    "    , 'optimizer': ['SGD']\n",
    "    , 'learning_rate': [0.01, 0.001, 0.10, 0.30]\n",
    "    , 'momentum': [0.0, 0.25, 0.5]\n",
    "    , 'rho': [99999]\n",
    "    , 'beta_1': [99999]\n",
    "    , 'beta_2': [99999]\n",
    "    , 'return_metrics': ['False']\n",
    "    , 'batch_size': [None, 32, 64, 128]\n",
    "    , 'epochs': [1, 10, 100, 500, 1000]\n",
    "},\n",
    "# Adadelta optimizer\n",
    "    {\n",
    "    'num_neurons': [math.floor((len(keep_num_vars_gini)+1)/2)]\n",
    "    , 'num_hidden_layers': [1, 2]\n",
    "    , 'input_dim': [data['data_{}'.format(sample_values_solution[0])][keep_num_vars_gini].shape[1]]\n",
    "    , 'kernel_initializer': ['normal', 'uniform']\n",
    "    , 'activation': ['relu', 'elu', 'linear', 'softplus']\n",
    "    , 'kernel_constraint': [0,1,2,3,4,5]\n",
    "    , 'dropout_rate': [0, 0.25, 0.5, 0.75]\n",
    "    , 'output_kernel_initializer': ['uniform']\n",
    "    , 'output_activation': ['sigmoid']\n",
    "    , 'loss': ['binary_crossentropy']\n",
    "    , 'optimizer': ['Adadelta']\n",
    "    , 'learning_rate': [1, 0.5, 0.7, 0.9]\n",
    "    , 'momentum': [0.0, 0.25, 0.5]\n",
    "    , 'rho': [0.95, 0.8, 0.9, 0.99]\n",
    "    , 'beta_1': [99999]\n",
    "    , 'beta_2': [99999]\n",
    "    , 'return_metrics': ['False']\n",
    "    , 'batch_size': [None, 32, 64, 128]\n",
    "    , 'epochs': [1, 10, 100, 500, 1000]\n",
    "}, \n",
    "# Adam optimizer\n",
    "    {\n",
    "    'num_neurons': [math.floor((len(keep_num_vars_gini)+1)/2)]\n",
    "    , 'num_hidden_layers': [1, 2]\n",
    "    , 'input_dim': [data['data_{}'.format(sample_values_solution[0])][keep_num_vars_gini].shape[1]]\n",
    "    , 'kernel_initializer': ['normal', 'uniform']\n",
    "    , 'activation': ['relu', 'elu', 'linear', 'softplus']\n",
    "    , 'kernel_constraint': [0,1,2,3,4,5]\n",
    "    , 'dropout_rate': [0, 0.25, 0.5, 0.75]\n",
    "    , 'output_kernel_initializer': ['uniform']\n",
    "    , 'output_activation': ['sigmoid']\n",
    "    , 'loss': ['binary_crossentropy']\n",
    "    , 'optimizer': ['Adam']\n",
    "    , 'learning_rate': [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "    , 'momentum': [99999]\n",
    "    , 'rho': [99999]\n",
    "    , 'beta_1': [0.9, 0.8, 0.95, 0.99]\n",
    "    , 'beta_2': [0.999, 0.8, 0.9, 0.95]\n",
    "    , 'return_metrics': ['False']\n",
    "    , 'batch_size': [None, 32, 64, 128]\n",
    "    , 'epochs': [1, 10, 100, 500, 1000]\n",
    "}, \n",
    "# Nadam optimizer\n",
    "    {\n",
    "    'num_neurons': [math.floor((len(keep_num_vars_gini)+1)/2)]\n",
    "    , 'num_hidden_layers': [1, 2]\n",
    "    , 'input_dim': [data['data_{}'.format(sample_values_solution[0])][keep_num_vars_gini].shape[1]]\n",
    "    , 'kernel_initializer': ['normal', 'uniform']\n",
    "    , 'activation': ['relu', 'elu', 'linear', 'softplus']\n",
    "    , 'kernel_constraint': [0,1,2,3,4,5]\n",
    "    , 'dropout_rate': [0, 0.25, 0.5, 0.75]\n",
    "    , 'output_kernel_initializer': ['uniform']\n",
    "    , 'output_activation': ['sigmoid']\n",
    "    , 'loss': ['binary_crossentropy']\n",
    "    , 'optimizer': ['Nadam']\n",
    "    , 'learning_rate': [0.002, 0.01, 0.1, 0.2]\n",
    "    , 'momentum': [99999]\n",
    "    , 'rho': [99999]\n",
    "    , 'beta_1': [0.9, 0.8, 0.95, 0.99]\n",
    "    , 'beta_2': [0.999, 0.8, 0.9, 0.95]\n",
    "    , 'return_metrics': ['False']\n",
    "    , 'batch_size': [None, 32, 64, 128]\n",
    "    , 'epochs': [1, 10, 100, 500, 1000]\n",
    "}, \n",
    "# Adamax optimizer\n",
    "    {\n",
    "    'num_neurons': [math.floor((len(keep_num_vars_gini)+1)/2)]\n",
    "    , 'num_hidden_layers': [1, 2]\n",
    "    , 'input_dim': [data['data_{}'.format(sample_values_solution[0])][keep_num_vars_gini].shape[1]]\n",
    "    , 'kernel_initializer': ['normal', 'uniform']\n",
    "    , 'activation': ['relu', 'elu', 'linear', 'softplus']\n",
    "    , 'kernel_constraint': [0,1,2,3,4,5]\n",
    "    , 'dropout_rate': [0, 0.25, 0.5, 0.75]\n",
    "    , 'output_kernel_initializer': ['uniform']\n",
    "    , 'output_activation': ['sigmoid']\n",
    "    , 'loss': ['binary_crossentropy']\n",
    "    , 'optimizer': ['Adamax']\n",
    "    , 'learning_rate': [0.002, 0.01, 0.1, 0.2]\n",
    "    , 'momentum': [99999]\n",
    "    , 'rho': [99999]\n",
    "    , 'beta_1': [0.9, 0.8, 0.95, 0.99]\n",
    "    , 'beta_2': [0.999, 0.8, 0.9, 0.95]\n",
    "    , 'return_metrics': ['False']\n",
    "    , 'batch_size': [None, 32, 64, 128]\n",
    "    , 'epochs': [1, 10, 100, 500, 1000]\n",
    "}\n",
    "]\n",
    "\n",
    "params=[{'num_neurons': 4, 'num_hidden_layers': 2, 'input_dim': data['data_{}'.format(sample_values_solution[0])][keep_num_vars_gini].shape[1], \n",
    "             'kernel_initializer': 'normal', 'activation': 'relu', 'kernel_constraint': 2, 'dropout_rate': 0, \n",
    "             'output_kernel_initializer': 'uniform', 'output_activation': 'sigmoid', 'loss': 'binary_crossentropy', \n",
    "             'optimizer': 'SGD', 'learning_rate': 0.01, 'momentum': 0, 'rho': 99999, 'beta_1': 99999, 'beta_2': 99999, \n",
    "             'return_metrics': 'False', 'batch_size': None, 'epochs': 1}\n",
    "        , {'num_neurons': 4, 'num_hidden_layers': 2, 'input_dim': data['data_{}'.format(sample_values_solution[0])][keep_num_vars_gini].shape[1], \n",
    "             'kernel_initializer': 'normal', 'activation': 'relu', 'kernel_constraint': 2, 'dropout_rate': 0, \n",
    "             'output_kernel_initializer': 'uniform', 'output_activation': 'sigmoid', 'loss': 'binary_crossentropy', \n",
    "             'optimizer': 'Adadelta', 'learning_rate': 1, 'momentum': 0.0, 'rho': 0.95, 'beta_1': 99999, 'beta_2': 99999, \n",
    "             'return_metrics': 'False', 'batch_size': None, 'epochs': 1}\n",
    "        , {'num_neurons': 4, 'num_hidden_layers': 2, 'input_dim': data['data_{}'.format(sample_values_solution[0])][keep_num_vars_gini].shape[1], \n",
    "             'kernel_initializer': 'normal', 'activation': 'relu', 'kernel_constraint': 2, 'dropout_rate': 0, \n",
    "             'output_kernel_initializer': 'uniform', 'output_activation': 'sigmoid', 'loss': 'binary_crossentropy', \n",
    "             'optimizer': 'Adam', 'learning_rate': 0.001, 'momentum': 99999, 'rho': 99999, 'beta_1': 0.9, 'beta_2': 0.999, \n",
    "             'return_metrics': 'False', 'batch_size': None, 'epochs': 1}\n",
    "        , {'num_neurons': 4, 'num_hidden_layers': 2, 'input_dim': data['data_{}'.format(sample_values_solution[0])][keep_num_vars_gini].shape[1], \n",
    "                     'kernel_initializer': 'normal', 'activation': 'relu', 'kernel_constraint': 2, 'dropout_rate': 0, \n",
    "                     'output_kernel_initializer': 'uniform', 'output_activation': 'sigmoid', 'loss': 'binary_crossentropy', \n",
    "                     'optimizer': 'Nadam', 'learning_rate': 0.002, 'momentum': 99999, 'rho': 99999, 'beta_1': 0.9, 'beta_2': 0.999, \n",
    "                     'return_metrics': 'False', 'batch_size': None, 'epochs': 1}\n",
    "        , {'num_neurons': 4, 'num_hidden_layers': 2, 'input_dim': data['data_{}'.format(sample_values_solution[0])][keep_num_vars_gini].shape[1], \n",
    "                     'kernel_initializer': 'normal', 'activation': 'relu', 'kernel_constraint': 2, 'dropout_rate': 0, \n",
    "                     'output_kernel_initializer': 'uniform', 'output_activation': 'sigmoid', 'loss': 'binary_crossentropy', \n",
    "                     'optimizer': 'Adamax', 'learning_rate': 0.002, 'momentum': 99999, 'rho': 99999, 'beta_1': 0.9, 'beta_2': 0.999, \n",
    "                     'return_metrics': 'False', 'batch_size': None, 'epochs': 1}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1c5493-093a-45f0-93d8-75c8f54cd615",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opt_params_dict, loss_dict = mb.step_search_weight(estimator=KerasClassifier, \n",
    "                                                   params=params, \n",
    "                                                   grid=grid, \n",
    "                                                   target=target_variable_name,\n",
    "                                                   weight=weight_variable_name, \n",
    "                                                   dev=data['data_{}'.format(sample_values_solution[0])], \n",
    "                                                   val=data['data_{}'.format(sample_values_solution[1])], \n",
    "                                                   keep=keep_num_vars_gini)\n",
    "\n",
    "print('\\n Best Parameters for each optimizer')\n",
    "print(opt_params_dict)\n",
    "print(loss_dict)\n",
    "\n",
    "loss = loss_dict[loss_dict.index(min(loss_dict))]\n",
    "opt_params = opt_params_dict[loss_dict.index(min(loss_dict))]\n",
    "print('\\n Best Parameters')\n",
    "print(opt_params)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a652f2-35de-47f6-875c-76aab84cb121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the top features\n",
    "feats_best_keras = ks_fn.top_keras_feat(dev_df=data['data_{}'.format(sample_values_solution[0])], \n",
    "                                        feats=keep_num_vars_gini, \n",
    "                                        target=target_variable_name, \n",
    "                                        weight_variable=weight_variable_name, \n",
    "                                        threshold=0.0001, \n",
    "                                        keras_function=ks_fn.neural_network_function_wrapper(\n",
    "    num_neurons_=[x[1] for x in list(opt_params.items()) if x[0]=='num_neurons'][0]\n",
    "    , num_hidden_layers_=[x[1] for x in list(opt_params.items()) if x[0]=='num_hidden_layers'][0]\n",
    "    , input_dim_=[x[1] for x in list(opt_params.items()) if x[0]=='input_dim'][0]\n",
    "    , kernel_initializer_=[x[1] for x in list(opt_params.items()) if x[0]=='kernel_initializer'][0]\n",
    "    , activation_=[x[1] for x in list(opt_params.items()) if x[0]=='activation'][0]\n",
    "    , kernel_constraint_=[x[1] for x in list(opt_params.items()) if x[0]=='kernel_constraint'][0]\n",
    "    , dropout_rate_=[x[1] for x in list(opt_params.items()) if x[0]=='dropout_rate'][0]\n",
    "    , output_kernel_initializer_=[x[1] for x in list(opt_params.items()) if x[0]=='output_kernel_initializer'][0]\n",
    "    , output_activation_=[x[1] for x in list(opt_params.items()) if x[0]=='output_activation'][0]\n",
    "    , loss_=[x[1] for x in list(opt_params.items()) if x[0]=='loss'][0]\n",
    "    , optimizer_=[x[1] for x in list(opt_params.items()) if x[0]=='optimizer'][0]\n",
    "    , learning_rate_=[x[1] for x in list(opt_params.items()) if x[0]=='learning_rate'][0]\n",
    "    , momentum_=[x[1] for x in list(opt_params.items()) if x[0]=='momentum'][0]\n",
    "    , rho_=[x[1] for x in list(opt_params.items()) if x[0]=='rho'][0]\n",
    "    , beta_1_=[x[1] for x in list(opt_params.items()) if x[0]=='beta_1'][0]\n",
    "    , beta_2_=[x[1] for x in list(opt_params.items()) if x[0]=='beta_2'][0]\n",
    "    , return_metrics_='True'), \n",
    "                epochs_=[x[1] for x in list(opt_params.items()) if x[0]=='epochs'][0]\n",
    "              , batch_size_=[x[1] for x in list(opt_params.items()) if x[0]=='batch_size'][0]\n",
    "            , feat_importance_num_display = 1000\n",
    "                                 )\n",
    "\n",
    "feats_best_keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206ff47c-a8af-435e-a9e8-364dc482fbb9",
   "metadata": {},
   "source": [
    "### Grid search based on iterative step search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1b1340-2c08-4b9a-9841-1c5f017489f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "grid = [\n",
    "# SGD optimizer\n",
    "    {\n",
    "    'num_neurons': [math.floor((len(feats_best_keras)+1)/2)]\n",
    "    , 'num_hidden_layers': [1, 2]\n",
    "    , 'input_dim': [data['data_{}'.format(sample_values_solution[0])][feats_best_keras].shape[1]]\n",
    "    , 'kernel_initializer': ['normal', 'uniform']\n",
    "    , 'activation': ['relu', 'elu', 'linear', 'softplus']\n",
    "    , 'kernel_constraint': [0,1,2,3,4,5]\n",
    "    , 'dropout_rate': [0, 0.25, 0.5, 0.75]\n",
    "    , 'output_kernel_initializer': ['uniform']\n",
    "    , 'output_activation': ['sigmoid']\n",
    "    , 'loss': ['binary_crossentropy']\n",
    "    , 'optimizer': ['SGD']\n",
    "    , 'learning_rate': [0.01, 0.001, 0.10, 0.30]\n",
    "    , 'momentum': [0.0, 0.25, 0.5]\n",
    "    , 'rho': [99999]\n",
    "    , 'beta_1': [99999]\n",
    "    , 'beta_2': [99999]\n",
    "    , 'return_metrics': ['False']\n",
    "    , 'batch_size': [None, 32, 64, 128]\n",
    "    , 'epochs': [1, 10, 100, 500, 1000]\n",
    "},\n",
    "# Adadelta optimizer\n",
    "    {\n",
    "    'num_neurons': [math.floor((len(feats_best_keras)+1)/2)]\n",
    "    , 'num_hidden_layers': [1, 2]\n",
    "    , 'input_dim': [data['data_{}'.format(sample_values_solution[0])][feats_best_keras].shape[1]]\n",
    "    , 'kernel_initializer': ['normal', 'uniform']\n",
    "    , 'activation': ['relu', 'elu', 'linear', 'softplus']\n",
    "    , 'kernel_constraint': [0,1,2,3,4,5]\n",
    "    , 'dropout_rate': [0, 0.25, 0.5, 0.75]\n",
    "    , 'output_kernel_initializer': ['uniform']\n",
    "    , 'output_activation': ['sigmoid']\n",
    "    , 'loss': ['binary_crossentropy']\n",
    "    , 'optimizer': ['Adadelta']\n",
    "    , 'learning_rate': [1, 0.5, 0.7, 0.9]\n",
    "    , 'momentum': [0.0, 0.25, 0.5]\n",
    "    , 'rho': [0.95, 0.8, 0.9, 0.99]\n",
    "    , 'beta_1': [99999]\n",
    "    , 'beta_2': [99999]\n",
    "    , 'return_metrics': ['False']\n",
    "    , 'batch_size': [None, 32, 64, 128]\n",
    "    , 'epochs': [1, 10, 100, 500, 1000]\n",
    "}, \n",
    "# Adam optimizer\n",
    "    {\n",
    "    'num_neurons': [math.floor((len(feats_best_keras)+1)/2)]\n",
    "    , 'num_hidden_layers': [1, 2]\n",
    "    , 'input_dim': [data['data_{}'.format(sample_values_solution[0])][feats_best_keras].shape[1]]\n",
    "    , 'kernel_initializer': ['normal', 'uniform']\n",
    "    , 'activation': ['relu', 'elu', 'linear', 'softplus']\n",
    "    , 'kernel_constraint': [0,1,2,3,4,5]\n",
    "    , 'dropout_rate': [0, 0.25, 0.5, 0.75]\n",
    "    , 'output_kernel_initializer': ['uniform']\n",
    "    , 'output_activation': ['sigmoid']\n",
    "    , 'loss': ['binary_crossentropy']\n",
    "    , 'optimizer': ['Adam']\n",
    "    , 'learning_rate': [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "    , 'momentum': [99999]\n",
    "    , 'rho': [99999]\n",
    "    , 'beta_1': [0.9, 0.8, 0.95, 0.99]\n",
    "    , 'beta_2': [0.999, 0.8, 0.9, 0.95]\n",
    "    , 'return_metrics': ['False']\n",
    "    , 'batch_size': [None, 32, 64, 128]\n",
    "    , 'epochs': [1, 10, 100, 500, 1000]\n",
    "}, \n",
    "# Nadam optimizer\n",
    "    {\n",
    "    'num_neurons': [math.floor((len(feats_best_keras)+1)/2)]\n",
    "    , 'num_hidden_layers': [1, 2]\n",
    "    , 'input_dim': [data['data_{}'.format(sample_values_solution[0])][feats_best_keras].shape[1]]\n",
    "    , 'kernel_initializer': ['normal', 'uniform']\n",
    "    , 'activation': ['relu', 'elu', 'linear', 'softplus']\n",
    "    , 'kernel_constraint': [0,1,2,3,4,5]\n",
    "    , 'dropout_rate': [0, 0.25, 0.5, 0.75]\n",
    "    , 'output_kernel_initializer': ['uniform']\n",
    "    , 'output_activation': ['sigmoid']\n",
    "    , 'loss': ['binary_crossentropy']\n",
    "    , 'optimizer': ['Nadam']\n",
    "    , 'learning_rate': [0.002, 0.01, 0.1, 0.2]\n",
    "    , 'momentum': [99999]\n",
    "    , 'rho': [99999]\n",
    "    , 'beta_1': [0.9, 0.8, 0.95, 0.99]\n",
    "    , 'beta_2': [0.999, 0.8, 0.9, 0.95]\n",
    "    , 'return_metrics': ['False']\n",
    "    , 'batch_size': [None, 32, 64, 128]\n",
    "    , 'epochs': [1, 10, 100, 500, 1000]\n",
    "}, \n",
    "# Adamax optimizer\n",
    "    {\n",
    "    'num_neurons': [math.floor((len(feats_best_keras)+1)/2)]\n",
    "    , 'num_hidden_layers': [1, 2]\n",
    "    , 'input_dim': [data['data_{}'.format(sample_values_solution[0])][feats_best_keras].shape[1]]\n",
    "    , 'kernel_initializer': ['normal', 'uniform']\n",
    "    , 'activation': ['relu', 'elu', 'linear', 'softplus']\n",
    "    , 'kernel_constraint': [0,1,2,3,4,5]\n",
    "    , 'dropout_rate': [0, 0.25, 0.5, 0.75]\n",
    "    , 'output_kernel_initializer': ['uniform']\n",
    "    , 'output_activation': ['sigmoid']\n",
    "    , 'loss': ['binary_crossentropy']\n",
    "    , 'optimizer': ['Adamax']\n",
    "    , 'learning_rate': [0.002, 0.01, 0.1, 0.2]\n",
    "    , 'momentum': [99999]\n",
    "    , 'rho': [99999]\n",
    "    , 'beta_1': [0.9, 0.8, 0.95, 0.99]\n",
    "    , 'beta_2': [0.999, 0.8, 0.9, 0.95]\n",
    "    , 'return_metrics': ['False']\n",
    "    , 'batch_size': [None, 32, 64, 128]\n",
    "    , 'epochs': [1, 10, 100, 500, 1000]\n",
    "}\n",
    "]\n",
    "\n",
    "params=[{'num_neurons': 4, 'num_hidden_layers': 2, 'input_dim': data['data_{}'.format(sample_values_solution[0])][feats_best_keras].shape[1], \n",
    "             'kernel_initializer': 'normal', 'activation': 'relu', 'kernel_constraint': 2, 'dropout_rate': 0, \n",
    "             'output_kernel_initializer': 'uniform', 'output_activation': 'sigmoid', 'loss': 'binary_crossentropy', \n",
    "             'optimizer': 'SGD', 'learning_rate': 0.01, 'momentum': 0, 'rho': 99999, 'beta_1': 99999, 'beta_2': 99999, \n",
    "             'return_metrics': 'False', 'batch_size': None, 'epochs': 1}\n",
    "        , {'num_neurons': 4, 'num_hidden_layers': 2, 'input_dim': data['data_{}'.format(sample_values_solution[0])][feats_best_keras].shape[1], \n",
    "             'kernel_initializer': 'normal', 'activation': 'relu', 'kernel_constraint': 2, 'dropout_rate': 0, \n",
    "             'output_kernel_initializer': 'uniform', 'output_activation': 'sigmoid', 'loss': 'binary_crossentropy', \n",
    "             'optimizer': 'Adadelta', 'learning_rate': 1, 'momentum': 0.0, 'rho': 0.95, 'beta_1': 99999, 'beta_2': 99999, \n",
    "             'return_metrics': 'False', 'batch_size': None, 'epochs': 1}\n",
    "        , {'num_neurons': 4, 'num_hidden_layers': 2, 'input_dim': data['data_{}'.format(sample_values_solution[0])][feats_best_keras].shape[1], \n",
    "             'kernel_initializer': 'normal', 'activation': 'relu', 'kernel_constraint': 2, 'dropout_rate': 0, \n",
    "             'output_kernel_initializer': 'uniform', 'output_activation': 'sigmoid', 'loss': 'binary_crossentropy', \n",
    "             'optimizer': 'Adam', 'learning_rate': 0.001, 'momentum': 99999, 'rho': 99999, 'beta_1': 0.9, 'beta_2': 0.999, \n",
    "             'return_metrics': 'False', 'batch_size': None, 'epochs': 1}\n",
    "        , {'num_neurons': 4, 'num_hidden_layers': 2, 'input_dim': data['data_{}'.format(sample_values_solution[0])][feats_best_keras].shape[1], \n",
    "                     'kernel_initializer': 'normal', 'activation': 'relu', 'kernel_constraint': 2, 'dropout_rate': 0, \n",
    "                     'output_kernel_initializer': 'uniform', 'output_activation': 'sigmoid', 'loss': 'binary_crossentropy', \n",
    "                     'optimizer': 'Nadam', 'learning_rate': 0.002, 'momentum': 99999, 'rho': 99999, 'beta_1': 0.9, 'beta_2': 0.999, \n",
    "                     'return_metrics': 'False', 'batch_size': None, 'epochs': 1}\n",
    "        , {'num_neurons': 4, 'num_hidden_layers': 2, 'input_dim': data['data_{}'.format(sample_values_solution[0])][feats_best_keras].shape[1], \n",
    "                     'kernel_initializer': 'normal', 'activation': 'relu', 'kernel_constraint': 2, 'dropout_rate': 0, \n",
    "                     'output_kernel_initializer': 'uniform', 'output_activation': 'sigmoid', 'loss': 'binary_crossentropy', \n",
    "                     'optimizer': 'Adamax', 'learning_rate': 0.002, 'momentum': 99999, 'rho': 99999, 'beta_1': 0.9, 'beta_2': 0.999, \n",
    "                     'return_metrics': 'False', 'batch_size': None, 'epochs': 1}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82c7d1a-c06f-4c84-9f0b-b758a8bdc89d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opt_params_dict, loss_dict = mb.step_search_weight(estimator=KerasClassifier, \n",
    "                                                   params=params, \n",
    "                                                   grid=grid, \n",
    "                                                   target=target_variable_name,\n",
    "                                                   weight=weight_variable_name, \n",
    "                                                   dev=data['data_{}'.format(sample_values_solution[0])], \n",
    "                                                   val=data['data_{}'.format(sample_values_solution[1])], \n",
    "                                                   keep=feats_best_keras)\n",
    "\n",
    "print('\\n Best Parameters for each optimizer')\n",
    "print(opt_params_dict)\n",
    "print(loss_dict)\n",
    "\n",
    "loss = loss_dict[loss_dict.index(min(loss_dict))]\n",
    "opt_params_iterative = opt_params_dict[loss_dict.index(min(loss_dict))]\n",
    "print('\\n Best Parameters')\n",
    "print(opt_params_iterative)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1ff5d8-065e-4b1c-9fe3-722a0683a4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load NN machine leanring library\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Define the Keras model\n",
    "nn = KerasClassifier(build_fn=ks_fn.neural_network_function_wrapper(\n",
    "    num_neurons_=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='num_neurons'][0]\n",
    "    , num_hidden_layers_=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='num_hidden_layers'][0]\n",
    "    , input_dim_=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='input_dim'][0]\n",
    "    , kernel_initializer_=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='kernel_initializer'][0]\n",
    "    , activation_=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='activation'][0]\n",
    "    , kernel_constraint_=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='kernel_constraint'][0]\n",
    "    , dropout_rate_=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='dropout_rate'][0]\n",
    "    , output_kernel_initializer_=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='output_kernel_initializer'][0]\n",
    "    , output_activation_=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='output_activation'][0]\n",
    "    , loss_=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='loss'][0]\n",
    "    , optimizer_=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='optimizer'][0]\n",
    "    , learning_rate_=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='learning_rate'][0]\n",
    "    , momentum_=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='momentum'][0]\n",
    "    , rho_=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='rho'][0]\n",
    "    , beta_1_=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='beta_1'][0]\n",
    "    , beta_2_=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='beta_2'][0]\n",
    "    , return_metrics_='True'), verbose=0)\n",
    "\n",
    "nn_model = nn.fit(data['data_{}'.format(sample_values_solution[0])][feats_best_keras].values, \n",
    "                  data['data_{}'.format(sample_values_solution[0])][target_variable_name], \n",
    "                  sample_weight=data['data_{}'.format(sample_values_solution[0])][weight_variable_name].values, \n",
    "                  use_multiprocessing=True, \n",
    "                  workers=8, \n",
    "                  verbose=0, \n",
    "                  epochs=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='epochs'][0], \n",
    "                  batch_size=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='batch_size'][0])\n",
    "\n",
    "# Compute the score for the OOT data\n",
    "for k in data.keys():\n",
    "    data[k].loc[:, 'nn_score_iterative_numeric'] = nn.predict_proba(x = data[k][feats_best_keras])[:, 1]\n",
    "    data[k].loc[:, 'nn_score_iterative_binary'] = list(map(round, data[k].loc[:, 'nn_score_iterative_numeric']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eef43e6-2e10-436d-9d26-c46bcad614e5",
   "metadata": {},
   "source": [
    "### Grid search based on RandomizedSearchCV - also uses cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebd8185-14e5-444e-bbbb-6e56bf5832d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# define the grid search parameters\n",
    "grid = [\n",
    "# SGD optimizer\n",
    "    {\n",
    "    'batch_size': [None, 32, 64, 128]\n",
    "    , 'epochs': [1, 10, 100, 500, 1000]\n",
    "    , 'num_neurons': [math.floor((len(feats_best_keras)+1)/2)]\n",
    "    , 'num_hidden_layers': [1, 2]\n",
    "    , 'input_dim': [data['data_{}'.format(sample_values_solution[0])][feats_best_keras].shape[1]]\n",
    "    , 'kernel_initializer': ['normal', 'uniform']\n",
    "    , 'activation': ['relu', 'elu', 'linear', 'softplus']\n",
    "    , 'kernel_constraint': [0,1,2,3,4,5]\n",
    "    , 'dropout_rate': [0, 0.25, 0.5, 0.75]\n",
    "    , 'output_kernel_initializer': ['uniform']\n",
    "    , 'output_activation': ['sigmoid']\n",
    "    , 'loss': ['binary_crossentropy']\n",
    "    , 'optimizer': ['SGD']\n",
    "    , 'learning_rate': [0.01, 0.001, 0.10, 0.30]\n",
    "    , 'momentum': [0.0, 0.25, 0.5]\n",
    "    , 'rho': [99999]\n",
    "    , 'beta_1': [99999]\n",
    "    , 'beta_2': [99999]\n",
    "    , 'return_metrics': ['False']\n",
    "},\n",
    "# Adadelta optimizer\n",
    "    {\n",
    "    'batch_size': [None, 32, 64, 128]\n",
    "    , 'epochs': [1, 10, 100, 500, 1000]\n",
    "    , 'num_neurons': [math.floor((len(feats_best_keras)+1)/2)]\n",
    "    , 'num_hidden_layers': [1, 2]\n",
    "    , 'input_dim': [data['data_{}'.format(sample_values_solution[0])][feats_best_keras].shape[1]]\n",
    "    , 'kernel_initializer': ['normal', 'uniform']\n",
    "    , 'activation': ['relu', 'elu', 'linear', 'softplus']\n",
    "    , 'kernel_constraint': [0,1,2,3,4,5]\n",
    "    , 'dropout_rate': [0, 0.25, 0.5, 0.75]\n",
    "    , 'output_kernel_initializer': ['uniform']\n",
    "    , 'output_activation': ['sigmoid']\n",
    "    , 'loss': ['binary_crossentropy']\n",
    "    , 'optimizer': ['Adadelta']\n",
    "    , 'learning_rate': [1, 0.5, 0.7, 0.9]\n",
    "    , 'momentum': [0.0, 0.25, 0.5]\n",
    "    , 'rho': [0.95, 0.8, 0.9, 0.99]\n",
    "    , 'beta_1': [99999]\n",
    "    , 'beta_2': [99999]\n",
    "    , 'return_metrics': ['False']\n",
    "}, \n",
    "# Adam optimizer\n",
    "    {\n",
    "    'batch_size': [None, 32, 64, 128]\n",
    "    , 'epochs': [1, 10, 100, 500, 1000]\n",
    "    , 'num_neurons': [math.floor((len(feats_best_keras)+1)/2)]\n",
    "    , 'num_hidden_layers': [1, 2]\n",
    "    , 'input_dim': [data['data_{}'.format(sample_values_solution[0])][feats_best_keras].shape[1]]\n",
    "    , 'kernel_initializer': ['normal', 'uniform']\n",
    "    , 'activation': ['relu', 'elu', 'linear', 'softplus']\n",
    "    , 'kernel_constraint': [0,1,2,3,4,5]\n",
    "    , 'dropout_rate': [0, 0.25, 0.5, 0.75]\n",
    "    , 'output_kernel_initializer': ['uniform']\n",
    "    , 'output_activation': ['sigmoid']\n",
    "    , 'loss': ['binary_crossentropy']\n",
    "    , 'optimizer': ['Adam']\n",
    "    , 'learning_rate': [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "    , 'momentum': [99999]\n",
    "    , 'rho': [99999]\n",
    "    , 'beta_1': [0.9, 0.8, 0.95, 0.99]\n",
    "    , 'beta_2': [0.999, 0.8, 0.9, 0.95]\n",
    "    , 'return_metrics': ['False']\n",
    "}, \n",
    "# Nadam optimizer\n",
    "    {\n",
    "    'batch_size': [None, 32, 64, 128]\n",
    "    , 'epochs': [1, 10, 100, 500, 1000]\n",
    "    , 'num_neurons': [math.floor((len(feats_best_keras)+1)/2)]\n",
    "    , 'num_hidden_layers': [1, 2]\n",
    "    , 'input_dim': [data['data_{}'.format(sample_values_solution[0])][feats_best_keras].shape[1]]\n",
    "    , 'kernel_initializer': ['normal', 'uniform']\n",
    "    , 'activation': ['relu', 'elu', 'linear', 'softplus']\n",
    "    , 'kernel_constraint': [0,1,2,3,4,5]\n",
    "    , 'dropout_rate': [0, 0.25, 0.5, 0.75]\n",
    "    , 'output_kernel_initializer': ['uniform']\n",
    "    , 'output_activation': ['sigmoid']\n",
    "    , 'loss': ['binary_crossentropy']\n",
    "    , 'optimizer': ['Nadam']\n",
    "    , 'learning_rate': [0.002, 0.01, 0.1, 0.2]\n",
    "    , 'momentum': [99999]\n",
    "    , 'rho': [99999]\n",
    "    , 'beta_1': [0.9, 0.8, 0.95, 0.99]\n",
    "    , 'beta_2': [0.999, 0.8, 0.9, 0.95]\n",
    "    , 'return_metrics': ['False']\n",
    "}, \n",
    "# Adamax optimizer\n",
    "    {\n",
    "    'batch_size': [None, 32, 64, 128]\n",
    "    , 'epochs': [1, 10, 100, 500, 1000]\n",
    "    , 'num_neurons': [math.floor((len(feats_best_keras)+1)/2)]\n",
    "    , 'num_hidden_layers': [1, 2]\n",
    "    , 'input_dim': [data['data_{}'.format(sample_values_solution[0])][feats_best_keras].shape[1]]\n",
    "    , 'kernel_initializer': ['normal', 'uniform']\n",
    "    , 'activation': ['relu', 'elu', 'linear', 'softplus']\n",
    "    , 'kernel_constraint': [0,1,2,3,4,5]\n",
    "    , 'dropout_rate': [0, 0.25, 0.5, 0.75]\n",
    "    , 'output_kernel_initializer': ['uniform']\n",
    "    , 'output_activation': ['sigmoid']\n",
    "    , 'loss': ['binary_crossentropy']\n",
    "    , 'optimizer': ['Adamax']\n",
    "    , 'learning_rate': [0.002, 0.01, 0.1, 0.2]\n",
    "    , 'momentum': [99999]\n",
    "    , 'rho': [99999]\n",
    "    , 'beta_1': [0.9, 0.8, 0.95, 0.99]\n",
    "    , 'beta_2': [0.999, 0.8, 0.9, 0.95]\n",
    "    , 'return_metrics': ['False']\n",
    "}\n",
    "]\n",
    "\n",
    "kerasnn_grid_random_search = mb.grid_search_cv(\n",
    "    n_splits = 2, # Number of cross-validation splits\n",
    "    classifier = KerasClassifier, # Classifier name, e.g. RandomForestClassifier\n",
    "    keras_function = ks_fn.neural_network_function_wrapper(\n",
    "    num_neurons_=20\n",
    "    , num_hidden_layers_=1\n",
    "    , input_dim_=data['data_{}'.format(sample_values_solution[0])][keep_num_vars_gini].shape[1]\n",
    "    , kernel_initializer_='normal'\n",
    "    , activation_='relu'\n",
    "    , kernel_constraint_=0\n",
    "    , dropout_rate_=0.5\n",
    "    , output_kernel_initializer_='normal'\n",
    "    , output_activation_='sigmoid'\n",
    "    , loss_='binary_crossentropy'\n",
    "    , optimizer_='Adam'\n",
    "    , learning_rate_=0.01\n",
    "    , momentum_=0.0\n",
    "    , rho_=0.95\n",
    "    , beta_1_=0.9\n",
    "    , beta_2_=0.999\n",
    "    , return_metrics_='False'\n",
    "),\n",
    "    grid_params = grid, # Grid space\n",
    "    dev_df = data['data_{}'.format(sample_values_solution[0])],  # Development sample that this will analysis will be performed\n",
    "    feats = feats_best_keras, # List of predictor names\n",
    "    target = target_variable_name, # Target variable name\n",
    "    weight_variable = weight_variable_name, # Weight variable name\n",
    "    randomized_search = True, # Set to True if randomized grid search will be performed, or to False if exhaustive grid search will be performed\n",
    "    n_random_grids = 50, # Number of grid searches when randomized_search=True. If randomized_search=False, then this parameter is not applicable\n",
    "    random_state = None, # If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.\n",
    "    n_jobs = 8 # Number of jobs to run in parallel\n",
    ")\n",
    "\n",
    "opt_params_fixed = kerasnn_grid_random_search.best_params_\n",
    "print('The best hyperparameter set is:', opt_params_fixed)\n",
    "print('Mean loss function for cross-validation test data: ', -kerasnn_grid_random_search.cv_results_['mean_test_score'][kerasnn_grid_random_search.best_index_])\n",
    "print('Standard deviation loss function for cross-validation test data: ', kerasnn_grid_random_search.cv_results_['std_test_score'][kerasnn_grid_random_search.best_index_])\n",
    "\n",
    "rp.plot_cross_validation_score(model=kerasnn_grid_random_search)\n",
    "\n",
    "# Compute the score for the OOT data\n",
    "for k in data.keys():\n",
    "    data[k].loc[:, 'nn_score_random_numeric'] = kerasnn_grid_random_search.predict_proba(X = data[k][feats_best_keras])[:, 1]\n",
    "    data[k].loc[:, 'nn_score_random_binary'] = list(map(round, data[k].loc[:, 'nn_score_random_numeric']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2274eb-8b0e-4f90-a2f2-0a133c3696f1",
   "metadata": {},
   "source": [
    "### Grid search based on AutoKeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92437c75-6b03-4c5e-94b2-1cef521c9f41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DRAWBACKS\n",
    "# 1) Autokeras does not optimize batch_size and epochs\n",
    "# 2) Autokeras cannot incorporate weights\n",
    "\n",
    "import timeit\n",
    "start = timeit.default_timer()\n",
    "\n",
    "import autokeras as ak\n",
    "\n",
    "# Set the number of iterations\n",
    "num_trials = 2\n",
    "\n",
    "# Initialize the structured data classifier.\n",
    "clf = ak.StructuredDataClassifier(column_names=feats_best_keras, loss='binary_crossentropy'\n",
    "                                  , max_trials=num_trials\n",
    "                                 , directory=r'C:\\Data_Science123'                                  \n",
    ") \n",
    "\n",
    "###############################################################################################################\n",
    "# Data standardization with sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# create scaler\n",
    "scaler = StandardScaler()\n",
    "###############################################################################################################\n",
    "# Data normalization with sklearn\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "# create scaler\n",
    "#scaler = MinMaxScaler()\n",
    "###############################################################################################################\n",
    "# fit scaler on data\n",
    "scaler.fit(data['data_{}'.format(sample_values_solution[0])][feats_best_keras].values.astype('float32'))\n",
    "# apply transform\n",
    "x_train = scaler.transform(data['data_{}'.format(sample_values_solution[0])][feats_best_keras].values.astype('float32'))\n",
    "x_test = scaler.transform(data['data_{}'.format(sample_values_solution[1])][feats_best_keras].values.astype('float32'))\n",
    "\n",
    "y_train = data['data_{}'.format(sample_values_solution[0])][target_variable_name]\n",
    "y_test = data['data_{}'.format(sample_values_solution[1])][target_variable_name]\n",
    "\n",
    "# Feed the structured data classifier with training data.\n",
    "clf.fit(x=x_train, y=y_train\n",
    "#        , sample_weight=data['data_{}'.format(sample_values_solution[0])][weight_variable_name].values\n",
    "       , use_multiprocessing=True, workers=8, verbose=0\n",
    "        , validation_data=(data['data_{}'.format(sample_values_solution[1])][feats_best_keras].values, \n",
    "                           data['data_{}'.format(sample_values_solution[1])][target_variable_name])\n",
    "#              , epochs=[x[1] for x in list(opt_params_fixed.items()) if x[0]=='epochs'][0]\n",
    "#              , batch_size=[x[1] for x in list(opt_params_fixed.items()) if x[0]=='batch_size'][0]       \n",
    "       )\n",
    "\n",
    "# Evaluate the best model with testing data.\n",
    "autokeras_loss, autokeras_accuracy = clf.evaluate(x=data['data_{}'.format(sample_values_solution[1])][feats_best_keras].values, y=data['data_{}'.format(sample_values_solution[1])][target_variable_name], verbose=0)\n",
    "print('Loss: %.3f' % autokeras_loss)\n",
    "print('Accuracy: %.3f' % autokeras_accuracy)\n",
    "\n",
    "# Evaluate the Auto-Keras model\n",
    "from sklearn.metrics import classification_report\n",
    "predictions = clf.predict(x=data['data_{}'.format(sample_values_solution[1])][feats_best_keras])\n",
    "report = classification_report(data['data_{}'.format(sample_values_solution[1])][target_variable_name], predictions)\n",
    "print(report)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Execution time in seconds: ', stop - start)  \n",
    "#Execution time in seconds:  202.19050749999587"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a43792-92d7-4e05-b9fd-51a9a33b81c3",
   "metadata": {},
   "source": [
    "## Calculate NN feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9833e7-20cf-4028-8e82-a99367b46437",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_fn.top_keras_feat(dev_df=data['data_{}'.format(sample_values_solution[0])], \n",
    "                     feats=feats_best_keras, \n",
    "                     target=target_variable_name, \n",
    "                     weight_variable=weight_variable_name, \n",
    "                     threshold=0, keras_function=ks_fn.neural_network_function_wrapper(\n",
    "    num_neurons_=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='num_neurons'][0]\n",
    "    , num_hidden_layers_=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='num_hidden_layers'][0]\n",
    "    , input_dim_=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='input_dim'][0]\n",
    "    , kernel_initializer_=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='kernel_initializer'][0]\n",
    "    , activation_=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='activation'][0]\n",
    "    , kernel_constraint_=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='kernel_constraint'][0]\n",
    "    , dropout_rate_=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='dropout_rate'][0]\n",
    "    , output_kernel_initializer_=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='output_kernel_initializer'][0]\n",
    "    , output_activation_=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='output_activation'][0]\n",
    "    , loss_=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='loss'][0]\n",
    "    , optimizer_=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='optimizer'][0]\n",
    "    , learning_rate_=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='learning_rate'][0]\n",
    "    , momentum_=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='momentum'][0]\n",
    "    , rho_=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='rho'][0]\n",
    "    , beta_1_=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='beta_1'][0]\n",
    "    , beta_2_=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='beta_2'][0]\n",
    "    , return_metrics_='True'), \n",
    "                epochs_=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='epochs'][0]\n",
    "              , batch_size_=[x[1] for x in list(opt_params_iterative.items()) if x[0]=='batch_size'][0]\n",
    "            , feat_importance_num_display = 1000\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d3da1c-2b92-4608-a022-70c2ad808835",
   "metadata": {},
   "source": [
    "## Produce NN reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdacdc63-44a4-485c-8731-67b1ce84fe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_iterative_report_class = rp.binary_regression_report(\n",
    "    predictions_dictionary = data, \n",
    "    target_variable = target_variable_name, \n",
    "    predicted_score_numeric = 'nn_score_iterative_numeric', \n",
    "    amount_variable_name = amount_variable_name_solution, \n",
    "    weight_variable_name = weight_variable_name_solution, \n",
    "    sample_values_dict = sample_values_dict, \n",
    "    select_top_percent = 100, \n",
    "    n_bands = 10, \n",
    "    rows = 10, \n",
    "    data_path = data_path\n",
    "    )\n",
    "nn_iterative_eval = nn_iterative_report_class.get_evaluation(predicted_score_binary = 'nn_score_iterative_binary', \n",
    "                                                       filename = 'evaluation_metrics_NN_iterative.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c2bd1f-ea0b-4c9a-86c7-ab608383ae83",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_random_report_class = rp.binary_regression_report(\n",
    "    predictions_dictionary = data, \n",
    "    target_variable = target_variable_name, \n",
    "    predicted_score_numeric = 'nn_score_random_numeric', \n",
    "    amount_variable_name = amount_variable_name_solution, \n",
    "    weight_variable_name = weight_variable_name_solution, \n",
    "    sample_values_dict = sample_values_dict, \n",
    "    select_top_percent = 100, \n",
    "    n_bands = 10, \n",
    "    rows = 10, \n",
    "    data_path = data_path\n",
    "    )\n",
    "nn_random_eval = nn_random_report_class.get_evaluation(predicted_score_binary = 'nn_score_random_binary', \n",
    "                                                       filename = 'evaluation_metrics_NN_random.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3783bf-894c-4335-89fe-84160bdd8d96",
   "metadata": {},
   "source": [
    "## Calculate NN Lifting table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd73a6aa-60aa-4084-a553-6c92c82b0638",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_report_class = nn_random_report_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eb81cb-8452-46f9-9708-d84893a5e617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Lift table\n",
    "binary_report_lift_table = binary_report_class.create_lift_table(filename = 'lift_table_NN_')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ca453c-d310-486d-a22e-ec9b9a5c16b5",
   "metadata": {},
   "source": [
    "## Plots NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b852d0f5-ff76-4941-9512-cc2a541bbbf3",
   "metadata": {},
   "source": [
    "### Plot NN Detection rate vs. Population Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c91206-4924-445e-9f80-771a08fc2fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder, if it doesn't exist\n",
    "folder_name = 'graphs_NN'\n",
    "ufun.create_folder(data_path = data_path, \n",
    "                   folder_name = 'output/{}'.format(folder_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1221511-570b-465a-b35c-83a85614c9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_report_class.plot_ADR_Quantile(\n",
    "        folder_name = folder_name,\n",
    "        xlim=None, \n",
    "        ylim=None\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ebb11b-5fe5-4b01-ab44-befa95ec2d5b",
   "metadata": {},
   "source": [
    "### Plot NN Cum. Detection rate vs. Population Distribution (Gains chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24580fd6-e007-4817-8e46-5bd1f5ffe2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_report_class.plot_cADR_Quantile(\n",
    "        folder_name = folder_name,\n",
    "        xlim=None, \n",
    "        ylim=None\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c353a7-1add-45da-b7e9-9c8b39d4c3a9",
   "metadata": {},
   "source": [
    "### Plot NN FPR vs. Population Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926cc8a0-010a-486e-b9ba-8ca8daffefb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_report_class.plot_FPR_Quantile(\n",
    "        folder_name = folder_name,\n",
    "        xlim=None, \n",
    "        ylim=None\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7c5804-03ef-4b0b-bc28-0b644c3aab89",
   "metadata": {},
   "source": [
    "### Plot NN Cum. FPR vs. Population Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8049dae-0949-4b07-bb42-ad1834070e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_report_class.plot_cFPR_Quantile(\n",
    "        folder_name = folder_name,\n",
    "        xlim=None, \n",
    "        ylim=None\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6151e42-7359-4bde-b0f9-f675d54381ca",
   "metadata": {},
   "source": [
    "### Plot NN ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1427b609-9849-4f10-b508-870e0e905962",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_report_class.plot_ROC_curve(folder_name = folder_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c275edf-714e-4350-b7b2-30f020c011a6",
   "metadata": {},
   "source": [
    "### Plot NN Precision Recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5131702b-d288-43e5-a594-cf9403706459",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_report_class.plot_precision_recall_curve(folder_name = folder_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756f8cb7-35d8-4a45-9efe-dd547419edee",
   "metadata": {},
   "source": [
    "### Plot NN F1 score, Accuracy, Sensitivity, Specificity, Precision vs Cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295ab3a6-629d-4c69-ad45-91eb503d3357",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_report_class.plot_cutoffs(\n",
    "        folder_name = folder_name,\n",
    "        n_bands = 100, # Number of bands between 0 and 1\n",
    "        return_table=False # Set to True in order to return the table that produced the graph, otherwise set to False\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Supervised Modeling Solution ML",
   "language": "python",
   "name": "supervised_modeling_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
