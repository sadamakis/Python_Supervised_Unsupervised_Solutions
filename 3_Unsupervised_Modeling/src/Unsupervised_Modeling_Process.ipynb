{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd822763-a892-462e-a88d-66f27f0fb92c",
   "metadata": {},
   "source": [
    "# LIMITATIONS: Where weights are currently not implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946cd953-b695-4ea1-9754-8778f9f92bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Standardization\n",
    "#- Unsupervised metrics like silhouette score\n",
    "#- Factor Analysis\n",
    "#- PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811a4d67-7a58-4876-809d-0d458d5af588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the notebook full screen\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e4a306-c188-4322-b9dc-a1945462d1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import importlib\n",
    "#os.getcdw()\n",
    "#code_dir = os.path.dirname(os.getcdw())\n",
    "#project_dir = os.path.dirname(os.path.dirname(os.getcdw()))\n",
    "#data_path = os.path.join(code_dir, 'data')\n",
    "#functions_path = os.path.join(project_dir, 'functions')\n",
    "code_dir = r'C:\\Users\\creep\\Anaconda\\Sotiris_Solutions\\3_Unsupervised_Modeling\\src'\n",
    "project_dir = r'C:\\Users\\creep\\Anaconda\\Sotiris_Solutions\\3_Unsupervised_Modeling'\n",
    "data_path = r'C:\\Users\\creep\\Anaconda\\Sotiris_Solutions\\3_Unsupervised_Modeling\\data'\n",
    "functions_path = r'C:\\Users\\creep\\Anaconda\\Sotiris_Solutions\\functions'\n",
    "print(code_dir)\n",
    "print(project_dir)\n",
    "print(data_path)\n",
    "print(functions_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481f4f52-0696-4ae1-bc95-872a27ff04d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Python modules\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48c4c50-c0a9-4bb8-bbdd-3a3577db63e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path for the library\n",
    "import sys\n",
    "sys.path.insert(0, functions_path)\n",
    "import variable_reduction as vr\n",
    "from solution_steps import color\n",
    "from data_quality_report import dq_report\n",
    "import data_processing as cpd\n",
    "import dimensionality_reduction as dr\n",
    "import profiling as pro\n",
    "import factor_analysis\n",
    "import select_model as sm\n",
    "import feature_importance as fi\n",
    "from load_data import load_data\n",
    "import solution_steps as ss\n",
    "import json\n",
    "from sklearn.cluster import KMeans, DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba69179b-c611-4e48-81b9-288704db5818",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7eb9ff6-092c-4045-b77b-cbfdd806f4a9",
   "metadata": {},
   "source": [
    "# Initialize the solution variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79160fd2-bbd1-4867-8e81-a5ce75059076",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(project_dir, 'data/input/Unsupervised_Modeling_Solution_Input.json')) as f:\n",
    "    inputs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97136bd-4dae-4125-8fec-d25b1520dcb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba251a6-b67b-420e-a894-857de08b1b4b",
   "metadata": {},
   "source": [
    "## Essential parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b5b784-ed72-4cb5-8665-88243a6cb2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# String. Specify how to load the data. Options: csv, parq.\n",
    "Load_from = inputs[\"Load_from\"]\n",
    "# String. Specify the data location: this is the folder where the data for this project are saved. \n",
    "data_location = inputs[\"data_location\"]\n",
    "# String. Set the input data file. \n",
    "table_name = inputs[\"table_name\"]\n",
    "# Float. Number between 0-1 determining what percent of data to subsample. \n",
    "sample = float(inputs[\"sample\"])\n",
    "# String. Set the weight variable name in the original dataset. If not avaulable, then provide \"None\".\n",
    "weight_variable_name = inputs[\"weight_variable_name\"]\n",
    "# String. Set the sample column that has sample information, e.g. train/test/OOT or segment information, and will be used to split the data in different samples\n",
    "# If this column does not exist, then provide \"None\".\n",
    "sample_variable_name = inputs[\"sample_variable_name\"]\n",
    "# List of strings. Set the sub-sample values that are in the sample_variable_name field, e.f. for train/test data split and/or for different segments. \n",
    "# All samples defined in this parameters will be picked up by the solution and results will be created for these samples. \n",
    "# If sample column does not exist, then provide '[None]' (without quotes).\n",
    "sample_values = inputs[\"sample_values\"]\n",
    "# List. Provide the feature names for the numeric variables that will be used for clustering. \n",
    "numeric_variables_for_clustering = inputs[\"numeric_variables_clustering\"]\n",
    "# List. Provide the feature names for the character variables that will be used for clustering. \n",
    "character_variables_for_clustering = inputs[\"character_variables_clustering\"]\n",
    "# List. Provide the feature names for the numeric variables that will be used for profiling/overlaying. \n",
    "numeric_variables_for_profiling = inputs[\"numeric_variables_profiling\"]\n",
    "# List. Provide the feature names for the character variables that will be used for profiling/overlaying. \n",
    "character_variables_for_profiling = inputs[\"character_variables_profiling\"]\n",
    "# Int. Used in factor_analysis.remove_features function. Determines the number of factors to be used in Factor Analysis. \n",
    "number_factors = inputs[\"number_factors\"]\n",
    "# Int. Used in dimensionality_reduction.fit_transform. Determines the number of principal components to be used in the final PCA model. \n",
    "number_pcs = inputs[\"number_pcs\"]\n",
    "# Int. Used in dimensionality_reduction.fit_transform. Determines the number of principal components to be used in the final PCA model during the second iteration, after dropping features with low feautre importnance. \n",
    "number_pcs_2 = inputs[\"number_pcs_2\"]\n",
    "# List. Models to test. Must include the model function, fedault arguments, the test argument name, and a list of values to test. \n",
    "models_to_test = eval(inputs[\"models_to_test\"])\n",
    "# Dictionary. Includes the model function and arguments for the final model to be used in clustering. \n",
    "final_model = eval(inputs[\"final_model\"])\n",
    "# Dictionary. Includes the model function and arguments for the final model to be used in clustering, after features were dropped due to low feature importance. \n",
    "final_model_2 = eval(inputs[\"final_model_2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba96f056-7353-42c3-bb1f-1be4c34714a0",
   "metadata": {},
   "source": [
    "## Advanced parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4a8d27-1b3f-438e-8485-69d867daf5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Float. Takes values between 0 and 1. Used in 'select_missing_variables_to_drop' function. Variables with percentage missing values above this threshold will be \n",
    "# dropped from the rest of the process. \n",
    "select_missing_variables_to_drop_threshold = inputs[\"select_missing_variables_to_drop_threshold\"]\n",
    "# Integer. Used in 'character_classification' function. Character variables with more levels than this threshold will be dropped from the rest of the process. \n",
    "character_classification_threshold = inputs[\"character_classification_threshold\"]\n",
    "# Float. Used in the 'replace_outliers' function in the outlier removal section. This is the coefficient for Interquantile range. \n",
    "# It can be used to adjust how many outliers to replace; the higher the value the less outliers are replaced. \n",
    "iqr_coef = inputs[\"iqr_coef\"]\n",
    "# String. Used in 'impute_missing' class. Select the stratefy to impute the missing values. Current options are \"median\", \"mean\", \n",
    "# or a specific value without quotes, e.g. 0.\n",
    "impute_missing_imputation_strategy = inputs[\"impute_missing_imputation_strategy\"]\n",
    "# Float. Used in 'corr_eliminator' function in the initial correlations calculations. Variables with correlation greater than this threshold will be dropped. \n",
    "corr_threshold = inputs[\"corr_threshold\"]\n",
    "# Int. Used in the 'corr_eliminator' function in the initial correlations calculations. After highly correlated features are dropped, this is the number of the next highest correlations. \n",
    "top_n = eval(inputs[\"top_n\"])\n",
    "# Float. Used in factor_analysis.setup function. Variables with KMO above this threshold will be tested in Factor Analysis. \n",
    "kmo_threshold = inputs[\"kmo_threshold\"]\n",
    "# Float. Used in factor_analysis.remove features function. Variables with factor loadings above this threshold will be dropped. \n",
    "loadings_threshold = inputs[\"loadings_threshold\"]\n",
    "# Float. Used in FeatureImportance.feature_importance_keep_vars. Variables that have feature importance less than this threshold will be dropped from clustering. \n",
    "feature_importance_threshold = inputs[\"feature_importance_threshold\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e438bde-3333-478d-93e1-5df10740b2b8",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24b4d66-7b86-4f49-a97a-9dfcd556b0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full = load_data(method = Load_from, \n",
    "                     data_path = data_location, \n",
    "                     table_name = table_name, \n",
    "                     sample = sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a47f3de-ccf2-45ee-8453-5d06735fc594",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_full.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3680f4f7-c64b-429c-bb80-12a0c862b584",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929d4911-7cc7-428f-8d88-45aaa2ed195f",
   "metadata": {},
   "source": [
    "# Create the Weight and Sample variables, if not available in the input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee51b3c-dd87-4976-b7e3-b87a8f102f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the weight variable\n",
    "data_full, weight_variable_name_solution = ss.weight_var_assignment(data_full = data_full, \n",
    "                                                                 weight_variable_name = weight_variable_name)\n",
    "\n",
    "# Create the sample variable\n",
    "data_full, sample_values_solution, sample_variable_name_solution = ss.sample_var_assignment(data_full = data_full, \n",
    "                                                                                         sample_variable_name = sample_variable_name, \n",
    "                                                                                           sample_values = sample_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05fa0d3-f687-4ff8-81b1-4bdcd384cb5f",
   "metadata": {},
   "source": [
    "# Convert variable data types based on user information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3d5749-f47c-448d-b01a-271a922f7e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert character variables for clustering\n",
    "data_full, character_variables_list_clustering = ss.convert_character_var(data_full = data_full, \n",
    "                                                        original_candidate_variables_character = character_variables_for_clustering,\n",
    "                                                        sample_variable_name_solution = sample_variable_name_solution)\n",
    "data_full, character_variables_list_profiling = ss.convert_character_var(data_full = data_full, \n",
    "                                                        original_candidate_variables_character = character_variables_for_profiling,\n",
    "                                                        sample_variable_name_solution = sample_variable_name_solution)\n",
    "character_variables_list = list(set(character_variables_list_clustering + character_variables_list_profiling))\n",
    "\n",
    "# Convert numeric variables for clustering\n",
    "data_full, numeric_variables_list_clustering = ss.convert_numeric_var(data_full = data_full, \n",
    "                                                        original_candidate_variables_numeric = numeric_variables_for_clustering,\n",
    "                                                        weight_variable_name_solution = weight_variable_name_solution, \n",
    "                                                        target_variable_name = '')\n",
    "data_full, numeric_variables_list_profiling = ss.convert_numeric_var(data_full = data_full, \n",
    "                                                        original_candidate_variables_numeric = numeric_variables_for_profiling,\n",
    "                                                        weight_variable_name_solution = weight_variable_name_solution, \n",
    "                                                        target_variable_name = '')\n",
    "numeric_variables_list = list(set(numeric_variables_list_clustering + numeric_variables_list_profiling))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec215d6e-1a0a-4cbb-9b28-85ba476a96f6",
   "metadata": {},
   "source": [
    "# Data quality report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee1bd6e-5c3a-4e03-a55d-200083ab4b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "dq = dq_report(df = data_full, \n",
    "                data_path = data_path, \n",
    "                variables = character_variables_list + numeric_variables_list, \n",
    "                weight_variable = weight_variable_name_solution, \n",
    "                dq_report_file = 'data_quality_report.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c670fb-a527-4ca5-aca8-59694138182f",
   "metadata": {},
   "source": [
    "# Split sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15870ef1-fbea-4446-af80-8aa295e65658",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, sample_values_dict = ss.split_sample_data(\n",
    "    data_full=data_full, \n",
    "    sample_values_solution=sample_values_solution, \n",
    "    sample_variable_name_solution=sample_variable_name_solution\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0229ddf9-035c-4e08-93aa-9c4fd57ce689",
   "metadata": {},
   "source": [
    "# Set the original candidate variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecc5066-89ef-4e75-8dd0-dc0eab583f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_variables_clustering = character_variables_for_clustering + numeric_variables_for_clustering\n",
    "print(color.BLUE + 'Original variables for clustering: ' + color.END + str(original_variables_clustering))\n",
    "original_variables_profiling = character_variables_for_profiling + numeric_variables_for_profiling\n",
    "print(color.BLUE + 'Original variables for profiling: ' + color.END + str(original_variables_profiling))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f35b0e-1824-4b26-a14b-34473e054d44",
   "metadata": {},
   "source": [
    "# Remove variables with high missing values percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e79733-2d94-49d9-bc96-ec499df6bdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables excluded from the non-predictive features: keys, target, sample, etc\n",
    "excluded_variables = [x for x in data['data_{}'.format(sample_values_solution[0])].columns if x not in original_variables_clustering]\n",
    "print(color.BLUE + 'Variables to be excluded: ' + color.END + str(excluded_variables))\n",
    "print()\n",
    "# Produce and save the missing values table to review\n",
    "missing_variables_table, missing_variables = ss.missing_values_vars(\n",
    "    sample_values_dict=sample_values_dict, \n",
    "    data_path=data_path, \n",
    "    data=data, \n",
    "    weight_variable_name_solution=weight_variable_name_solution, \n",
    "    select_missing_variables_to_drop_threshold=select_missing_variables_to_drop_threshold\n",
    "    )\n",
    "# Create the variables to remove: non-predictors + variables with too many missing information\n",
    "excluded_variables = excluded_variables + missing_variables\n",
    "print(color.BLUE + 'Variables to remove from the remainder of the analysis: ' + color.END + str(excluded_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0c0e14-df5c-4af2-b574-e65b13c658b5",
   "metadata": {},
   "source": [
    "# Remove character variables with many levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64546dd3-f228-401b-976c-ac6cd142e877",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_char_vars_levels = ss.character_var_levels(\n",
    "    data = data, \n",
    "    data_path = data_path, \n",
    "    sample_values_solution = sample_values_solution,\n",
    "    excluded_variables = excluded_variables, \n",
    "    character_classification_threshold = character_classification_threshold\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59351c5b-d5af-47f5-9247-92c3a1ad4a95",
   "metadata": {},
   "source": [
    "# Outlier replacement for numeric variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35df4d7d-3ef1-4f98-9735-39fa8c187267",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outlier_variables = [i for i in numeric_variables_list if i not in excluded_variables]\n",
    "data_full = cpd.replace_outliers(\n",
    "    input_data = data_full, \n",
    "    variables = outlier_variables, \n",
    "    weight_variable = weight_variable_name_solution, \n",
    "    data_path = data_path, \n",
    "    outlier_info_file = 'outlier_info.csv', \n",
    "    iqr_coef = iqr_coef\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf04806-73b8-4aa9-bd85-4b8642fb173b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split sample data\n",
    "data = {}\n",
    "for i, j in sample_values_dict.items():\n",
    "    start_time = time.time()\n",
    "    print(color.BOLD + color.PURPLE + color.UNDERLINE + j + color.END)\n",
    "    \n",
    "    data['data_{}'.format(i)] = data_full[data_full[sample_variable_name_solution]==i]\n",
    "    print('The shape is: ', data['data_{}'.format(i)].shape)\n",
    "    \n",
    "    print('This code took %.2fs. to run'%(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597c12b4-cd50-41b2-bcb7-d5cf21e704f0",
   "metadata": {},
   "source": [
    "# Convert categorical variables to binary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8979d9e-35ec-4711-89ca-0e5667ef5675",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpd.character_to_binary(\n",
    "    input_data = data_full, \n",
    "    input_variable_list = keep_char_vars_levels, \n",
    "    drop = 'last', # Specifies which value to drop from the one hot encoder. None will return binary variables for all categories. 'first' will drop the most populated category. 'last' will drop the less populated category. \n",
    "    protected_class_valid_values = None # Specifies accepted values for the protected class column. For non-protected class conversions use 'None'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbba151-f45b-4056-a21e-75ed156dd986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split sample data\n",
    "data = {}\n",
    "for i, j in sample_values_dict.items():\n",
    "    start_time = time.time()\n",
    "    print(color.BOLD + color.PURPLE + color.UNDERLINE + j + color.END)\n",
    "    \n",
    "    data['data_{}'.format(i)] = data_full[data_full[sample_variable_name_solution]==i]\n",
    "    print('The shape is: ', data['data_{}'.format(i)].shape)\n",
    "    \n",
    "    print('This code took %.2fs. to run'%(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bd46ba-2885-4baa-80da-b58efe6a85ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep all numeric variables, including those that were one-hot encoded\n",
    "keep_num_vars = cpd.identify_numeric_variables(input_data=data['data_{}'.format(sample_values_solution[0])])\n",
    "keep_num_vars = [x for x in keep_num_vars if x not in excluded_variables]\n",
    "print('Keeping the following variables: ', keep_num_vars)\n",
    "print(len(keep_num_vars))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a899eb-a3a8-4747-ad54-0c04fdbfdd1e",
   "metadata": {},
   "source": [
    "# Impute missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485a4c89-ac57-45b7-8533-ef232b067eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_with_missing_dict = {}\n",
    "for i, j in sample_values_dict.items():\n",
    "    start_time = time.time()\n",
    "    print(color.BOLD + color.PURPLE + color.UNDERLINE + j + color.END)\n",
    "    \n",
    "    variables_with_missing_dict['variables_with_missing_dict_{}'.format(i)] = cpd.select_missing_variables_to_drop(\n",
    "    data_path = data_path, \n",
    "    sample_name = j, \n",
    "    threshold = 0\n",
    "    )\n",
    "    \n",
    "    print('This code took %.2fs. to run'%(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd80a1e0-5945-41fd-8300-fa03fd8f3e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numeric features with missing values. Imputation will be applied to only these features, in order to improve the performance of the code. \n",
    "variables_with_missing = list(dict.fromkeys(sum(variables_with_missing_dict.values(), [])))\n",
    "num_variables_with_missing = [i for i in keep_num_vars if i in variables_with_missing]\n",
    "num_variables_with_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85479abb-c97f-42e2-82f4-6bb99a68c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values\n",
    "start_time = time.time()\n",
    "impute_missing = cpd.impute_missing(\n",
    "        variables = num_variables_with_missing, \n",
    "        imputation_strategy = impute_missing_imputation_strategy)\n",
    "impute_missing.imputation_fit_weight(\n",
    "        input_data = data['data_{}'.format(sample_values_solution[0])], \n",
    "        weight_variable = weight_variable_name_solution)\n",
    "\n",
    "for i, j in sample_values_dict.items():\n",
    "    impute_missing.imputation_transform(input_data = data['data_{}'.format(i)])\n",
    "\n",
    "print('This code took %.2fs. to run'%(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79673b4-8cc8-430e-92ab-b6eb2e7273af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check missing values for imputed variables\n",
    "for i, j in sample_values_dict.items():\n",
    "    start_time = time.time()\n",
    "    print(color.BOLD + color.PURPLE + color.UNDERLINE + j + color.END)\n",
    "\n",
    "    if num_variables_with_missing != []:\n",
    "        print(data['data_{}'.format(i)][num_variables_with_missing].apply\n",
    "              (lambda x: (sum(data['data_{}'.format(i)][x.isnull()][weight_variable_name_solution])\n",
    "                /sum(data['data_{}'.format(i)][weight_variable_name_solution])) * 100, axis=0).sort_values(ascending=False))\n",
    "    else: \n",
    "        print('There are no variables with missing values to impute')\n",
    "\n",
    "    print('This code took %.2fs. to run'%(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be7c26c-5ee9-4765-be57-13988e861dfd",
   "metadata": {},
   "source": [
    "# Drop numeric variables with only one value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a123358d-39c6-44c8-a727-d0f1321fba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_num_vars_one_v = ss.keep_num_variables_one_value(\n",
    "    keep_num_vars = keep_num_vars, \n",
    "    data_path = data_path, \n",
    "    dq_report = 'data_quality_report.csv'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15496f4a-b311-40cc-934c-498d39e59d46",
   "metadata": {},
   "source": [
    "# Select features for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780b050e-b993-4a57-b92c-8f7d9f2296ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_one_hot_list = []\n",
    "for i in character_variables_for_clustering:\n",
    "    keep_one_hot_list = keep_one_hot_list + [col for col in keep_num_vars_one_v if col.startswith(i)]\n",
    "keep_numeric_vars_list = [x for x in original_variables_clustering if x in keep_num_vars_one_v]\n",
    "keep_vars_for_clustering = keep_numeric_vars_list + keep_one_hot_list\n",
    "print(keep_vars_for_clustering)\n",
    "print(len(keep_vars_for_clustering))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ab2282-4cbe-402e-b073-41f0b10f17ad",
   "metadata": {},
   "source": [
    "# Remove highly correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39667c34-9fe1-4fa4-9c4a-1df95acee00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs = vr.calculate_correlations(\n",
    "    train_df = data['data_{}'.format(sample_values_solution[0])], \n",
    "    features = keep_vars_for_clustering, \n",
    "    corr_threshold = corr_threshold, \n",
    "    weight_variable_name = weight_variable_name_solution\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eed3b6-70e1-4b9c-b5b0-5caea4989faa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eliminated, remaining_predictors = vr.correlation_elimination(\n",
    "    method = 'correlation', \n",
    "    features = keep_vars_for_clustering, \n",
    "    train_df = data['data_{}'.format(sample_values_solution[0])], \n",
    "    data_path = data_path, \n",
    "    corr_threshold = corr_threshold, \n",
    "    top_n = top_n, \n",
    "    weight_variable_name = weight_variable_name_solution, \n",
    "    correlations = corrs\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643129f0-6926-4b17-b3d9-f4652dcd6005",
   "metadata": {},
   "source": [
    "# Standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aec22b8-cd85-4ceb-8a73-90f2682deb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_standardized = cpd.standardize_data(\n",
    "    input_data = data, \n",
    "    variables = remaining_predictors, \n",
    "    training_sample = 'data_{}'.format(sample_values_solution[0]), \n",
    "    data_path = data_path, \n",
    "    filename = 'standard_scaler.pkl'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7566dab-9538-4cc9-ac92-297c9e6dcf61",
   "metadata": {},
   "source": [
    "# Remove features using Factor Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967d251b-b5d0-4394-b115-01ad5ec246d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fa = factor_analysis.FactorAnalysis(\n",
    "        data = data_standardized, \n",
    "        training_sample = 'data_{}'.format(sample_values_solution[0]), \n",
    "        datapath = data_path, \n",
    "        filename = 'FactorAnalysis'\n",
    "    )\n",
    "fa.setup(kmo_threshold = kmo_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67741421-d780-4dd4-ba83-511fa2f79a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_standardized = fa.remove_features(\n",
    "        n_factors = number_factors, \n",
    "        loadings_threshold = loadings_threshold\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee17917-d2fa-4c83-96ce-a482e7ebfd24",
   "metadata": {},
   "source": [
    "# PCA transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20210dd-a897-4904-bc9a-60d6cc7ab0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension_reduction = dr.dimension_reduction(dic_of_dfs = data_standardized, \n",
    "        data_path = data_path, \n",
    "        training_sample = 'data_{}'.format(sample_values_solution[0])\n",
    "        )\n",
    "dimension_reduction.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1162f3-47c3-4079-b65e-52b8b0c586e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_data = dimension_reduction.fit_transform(pca_components = number_pcs, \n",
    "        solver = 'full', \n",
    "        filename = 'pca_model.pkl'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544f4f88-4143-4acb-a7ec-9e65c0576d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_data['data_{}'.format(sample_values_solution[0])].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b578b13-582c-4636-a505-1083a9e4a137",
   "metadata": {},
   "source": [
    "# Clustering methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9701eac-388b-4e6f-ba51-66f4ea2d7585",
   "metadata": {},
   "source": [
    "## Select the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c09e46-8bf9-4e29-9daa-cc4a7fa40cff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sm_object = sm.SelectModel(df = pca_data,\n",
    "        sample_values_solution = sample_values_solution, \n",
    "        weights = data['data_{}'.format(sample_values_solution[0])][weight_variable_name_solution], \n",
    "        data_path = data_path, \n",
    "        filename = 'ClusterProfile_'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a483a2ea-2c2e-4770-ba27-6e1e38d4047b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for m in models_to_test:\n",
    "    sm_object.set_test_model(m)\n",
    "    display(sm_object.get_profile(bootstraps = 10, \n",
    "        sample_size = 0.1\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0ef845-8982-4323-ab56-c8e6dc4ef9a2",
   "metadata": {},
   "source": [
    "# Develop the Clustering model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204fe879-e233-42c6-a33b-8ff9f4f48a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm_object.create_model(model_inputs = final_model, filename='model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd5fa78-b604-48b2-97bb-b1b8aa28f179",
   "metadata": {},
   "source": [
    "# Test the Clustering model on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069b3da9-9224-4fca-b93f-60d88c9d236f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_object.validate_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6995320-413e-4b5f-98e2-9357250e76fe",
   "metadata": {},
   "source": [
    "# Feature contribution to clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e34d56-e871-4d35-9f79-c74b203b9d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "character_variables_dropped = [x for x in character_variables_for_clustering if x not in keep_char_vars_levels]\n",
    "numeric_variables_dropped = [x for x in numeric_variables_for_clustering if x not in keep_vars_for_clustering]\n",
    "non_feature_imp = character_variables_dropped + numeric_variables_dropped + [sample_variable_name_solution, weight_variable_name_solution]\n",
    "variables_list_profiling = [col for col in numeric_variables_list_profiling + character_variables_list_profiling if col not in non_feature_imp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342ed0ac-a298-4265-a9cb-55d8462ccdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_object = fi.FeatureImportance(X = data['data_{}'.format(sample_values_solution[0])][variables_list_profiling], \n",
    "        labels = model.labels_, \n",
    "        weights = data['data_{}'.format(sample_values_solution[0])][weight_variable_name_solution], \n",
    "        data_path = data_path, \n",
    "        filename = 'FeatureImportance'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9643c9b0-4801-48db-8d9c-735577576bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "imps = fi_object.get_report()\n",
    "imps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e20362-0bc7-427d-be57-0fe806060222",
   "metadata": {},
   "source": [
    "# Keep features based on Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725e6c3f-fab7-4231-aa41-573c386e7ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_fi_vars = fi_object.feature_importance_keep_vars(\n",
    "        feature_importance_threshold = feature_importance_threshold\n",
    "        )\n",
    "keep_fi_vars = [x for x in remaining_predictors if x in keep_fi_vars]\n",
    "print(keep_fi_vars)\n",
    "print(len(keep_fi_vars))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894bdfec-0c1a-485e-8649-c16be99ec1f7",
   "metadata": {},
   "source": [
    "# Standardize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77d6f44-818c-42ff-ac48-11424f8c4131",
   "metadata": {},
   "source": [
    "### The steps below are optional, but recommended since there is an opportunity to remove the least important features from clustering as they add noise to the clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d88eaf-f3b9-41a0-ac0a-6047d0797b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_standardized = cpd.standardize_data(\n",
    "    input_data = data, \n",
    "    variables = keep_fi_vars, \n",
    "    training_sample = 'data_{}'.format(sample_values_solution[0]), \n",
    "    data_path = data_path, \n",
    "    filename = 'standard_scaler_2.pkl'\n",
    "    )    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc8a706-eacd-4340-8f22-3f6c43397e8e",
   "metadata": {},
   "source": [
    "# PCA transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d82329-265c-4e84-adaf-0857846210e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension_reduction = dr.dimension_reduction(dic_of_dfs = data_standardized, \n",
    "        data_path = data_path, \n",
    "        training_sample = 'data_{}'.format(sample_values_solution[0])\n",
    "        )\n",
    "dimension_reduction.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ddac8d-43e9-4452-a416-cfef2e19409a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_data = dimension_reduction.fit_transform(pca_components = min(number_pcs_2, data_standardized['data_{}'.format(sample_values_solution[0])].shape[1]), \n",
    "        solver = 'full', \n",
    "        filename = 'pca_model_2.pkl'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3d0b62-0e70-480c-84b0-11653ce7074d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_data['data_{}'.format(sample_values_solution[0])].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfa9fb3-50d8-48c3-9d42-a4f382c2911f",
   "metadata": {},
   "source": [
    "# Clustering methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad808e82-c960-4baa-99ae-4cc1078a0302",
   "metadata": {},
   "source": [
    "## Select the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b7fd79-61b2-4c4e-bc81-c6d86acc7858",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_object = sm.SelectModel(df = pca_data,\n",
    "        sample_values_solution = sample_values_solution, \n",
    "        weights = data['data_{}'.format(sample_values_solution[0])][weight_variable_name_solution], \n",
    "        data_path = data_path, \n",
    "        filename = 'ClusterProfile_2_'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f386c643-0aaf-494a-a80a-69375880f280",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for m in models_to_test:\n",
    "    sm_object.set_test_model(m)\n",
    "    display(sm_object.get_profile(bootstraps = 10, \n",
    "        sample_size = 0.1\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737ad9ec-9636-4561-8111-785fb350edae",
   "metadata": {},
   "source": [
    "# Develop the Clustering model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e693e279-8a8b-4386-b41e-0d0331efe826",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm_object.create_model(model_inputs = final_model_2, filename='model_2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4237a0ee-c34a-4516-8b45-ba733b2381ce",
   "metadata": {},
   "source": [
    "# Test the Clustering model on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167ed127-4921-4794-81a9-5bf1b78500e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_object.validate_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03358b4-42f6-4e78-915c-a204a27d2336",
   "metadata": {},
   "source": [
    "# Feature contribution to clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e859249-0fb7-4090-b0d5-adb72bf6d345",
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_object = fi.FeatureImportance(X = data['data_{}'.format(sample_values_solution[0])][variables_list_profiling], \n",
    "        labels = model.labels_, \n",
    "        weights = data['data_{}'.format(sample_values_solution[0])][weight_variable_name_solution], \n",
    "        data_path = data_path, \n",
    "        filename = 'FeatureImportance_2'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23760b3b-7be7-4e80-8af8-c9093a0a2a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "imps = fi_object.get_report()\n",
    "imps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68b34f9-30ba-41e6-9ece-c6222dbb6eaf",
   "metadata": {},
   "source": [
    "# Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fc4e34-d19b-4ad0-8c36-d6da7e7dd045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add cluster labels to the input data\n",
    "data_cluster = data['data_{}'.format(sample_values_solution[0])]\n",
    "data_cluster['cluster_labels'] = model.labels_\n",
    "data_cluster.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882d2b75-84b0-4eff-8737-50bfcaf75830",
   "metadata": {},
   "source": [
    "## Profile categorical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ccee2c-a60a-4e0d-9288-619423ba0b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "pro.character_summary_statistics(\n",
    "    table_name = data_cluster, \n",
    "    variable_list = [x for x in character_variables_list_profiling if x not in [sample_variable_name_solution]], \n",
    "    cluster_variable_name = 'cluster_labels', \n",
    "    weight_variable_name = weight_variable_name_solution,\n",
    "    data_path = data_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a245ab-2442-459d-b109-3f5520fb635e",
   "metadata": {},
   "source": [
    "## Profile numeric attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1355ba4a-2b4c-4dbd-98fb-992f735f7d4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pro.numeric_summary_statistics(\n",
    "    table_name = data_cluster, \n",
    "    variable_list = [x for x in numeric_variables_list_profiling if x not in [weight_variable_name_solution]], \n",
    "    cluster_variable_name = 'cluster_labels', \n",
    "    weight_variable_name = weight_variable_name_solution,\n",
    "    data_path = data_path\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Unsupervised Modeling Solution",
   "language": "python",
   "name": "unsupervised_modeling"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
